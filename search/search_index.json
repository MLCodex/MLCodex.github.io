{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre> <p> <p></p>"},{"location":"Machine%20Learning/different-distance-calculations/","title":"Exploring Different Distance Calculation Techniques","text":"<p>Distance calculation is a fundamental operation in various machine learning algorithms, and it plays a crucial role in tasks such as clustering, classification, and nearest neighbor search. In this blog, we will explore several distance calculation techniques commonly used in machine learning and provide Python examples to demonstrate their implementation. We will also provide the mathematical formulas for each distance metric.</p>"},{"location":"Machine%20Learning/different-distance-calculations/#different-types-of-distances","title":"Different Types of Distances","text":""},{"location":"Machine%20Learning/different-distance-calculations/#euclidean-distance","title":"Euclidean Distance","text":"<p>Euclidean distance is one of the most widely used distance metrics, calculating the straight-line distance between two points in a Euclidean space. The formula for Euclidean distance between two points A and B in n-dimensional space is:</p> \\[\\large d(A, B) = \\sqrt{\\sum_{i=1}^{n}(A_i - B_i)^2}\\] <p>Python example:</p> <pre><code>import numpy as np\n\ndef euclidean_distance(A, B):\n    return np.sqrt(np.sum((A - B) ** 2))\n\nA = np.array([1, 2, 3])\nB = np.array([4, 5, 6])\n\ndistance = euclidean_distance(A, B)\nprint(\"Euclidean Distance:\", distance)\n</code></pre>"},{"location":"Machine%20Learning/different-distance-calculations/#advantages","title":"Advantages","text":"<ol> <li> <p>Intuitive Interpretation: Euclidean distance is easy to understand and interpret. It measures the straight-line distance between two points, which aligns with our intuitive notion of distance in Euclidean space.</p> </li> <li> <p>Simplicity: Euclidean distance is relatively simple to calculate and implement. It involves the square root of the sum of squared differences between corresponding coordinates, making it computationally efficient.</p> </li> <li> <p>Popular and Widely Used: Euclidean distance is one of the most popular distance metrics and is extensively used in various fields, including machine learning, data mining, and spatial analysis. Many algorithms and techniques are built upon or rely on Euclidean distance.</p> </li> </ol>"},{"location":"Machine%20Learning/different-distance-calculations/#disadvantages","title":"Disadvantages","text":"<ol> <li> <p>Sensitive to Outliers: Euclidean distance is sensitive to outliers or extreme values in the dataset. Outliers can greatly affect the overall distance calculation, potentially leading to biased results.</p> </li> <li> <p>Limited Applicability to High-Dimensional Data: Euclidean distance becomes less effective in high-dimensional spaces. This phenomenon is known as the \"curse of dimensionality.\" In high-dimensional spaces, the distances between points tend to become more similar, diminishing the discriminative power of Euclidean distance.</p> </li> <li> <p>Assumptions of Linearity: Euclidean distance assumes linearity between features or dimensions. It assumes that each dimension contributes equally to the overall distance calculation. However, in some cases, this assumption may not hold, and certain dimensions may be more important than others.</p> </li> <li> <p>Not Robust to Feature Scaling: Euclidean distance is sensitive to differences in feature scales. If the scales of different features are not similar, the distance calculation can be dominated by the features with larger scales, leading to biased results. Feature scaling or normalization is often required to address this issue.</p> </li> </ol> <p>It's important to consider these advantages and disadvantages when using Euclidean distance in your analysis. Depending on the nature of your data and the specific problem at hand, alternative distance metrics or data preprocessing techniques may be more appropriate.</p>"},{"location":"Machine%20Learning/different-distance-calculations/#manhattan-distance","title":"Manhattan Distance","text":"<p>Manhattan distance, also known as city block distance or L1 distance, calculates the distance between two points by summing the absolute differences of their coordinates. The formula for Manhattan distance between two points A and B in n-dimensional space is:</p> \\[\\large d(A, B) = \\sum_{i=1}^{n}|A_i - B_i|\\] <p>Python code for Manhattan distance</p> <pre><code>import numpy as np\n\ndef manhattan_distance(A, B):\n    return np.sum(np.abs(A - B))\n\nA = np.array([1, 2, 3])\nB = np.array([4, 5, 6])\n\ndistance = manhattan_distance(A, B)\nprint(\"Manhattan Distance:\", distance)\n</code></pre>"},{"location":"Machine%20Learning/different-distance-calculations/#advantages_1","title":"Advantages","text":"<ol> <li> <p>Robustness to Outliers: Manhattan distance is less sensitive to outliers compared to Euclidean distance. Since it calculates the sum of absolute differences between coordinates, outliers have a limited impact on the overall distance calculation.</p> </li> <li> <p>Applicability to High-Dimensional Data: Manhattan distance performs relatively well in high-dimensional spaces compared to Euclidean distance. It is less affected by the \"curse of dimensionality\" and can provide meaningful distance measurements even when dealing with data with a large number of dimensions.</p> </li> <li> <p>Feature Selection: Manhattan distance can be useful in feature selection tasks. By considering the Manhattan distance between a feature vector and the origin, you can identify the most important features based on their contribution to the overall distance.</p> </li> <li> <p>Computational Efficiency: Manhattan distance involves a simple calculation of the sum of absolute differences, making it computationally efficient. It requires fewer computations compared to Euclidean distance, especially when dealing with large datasets.</p> </li> </ol>"},{"location":"Machine%20Learning/different-distance-calculations/#disadvantages_1","title":"Disadvantages","text":"<ol> <li> <p>Limited Interpretability: The interpretation of Manhattan distance is not as intuitive as Euclidean distance. It measures the distance as the sum of absolute differences in coordinates, which may not align with our intuitive notion of distance in Euclidean space.</p> </li> <li> <p>Sensitivity to Feature Scaling: Manhattan distance is sensitive to differences in feature scales. If the scales of different features are not similar, the distance calculation can be dominated by the features with larger scales, leading to biased results. Feature scaling or normalization is often required to address this issue.</p> </li> <li> <p>Assumptions of Equal Importance: Manhattan distance assumes equal importance for each dimension or feature. However, in some cases, certain dimensions may be more important than others, and this assumption may not hold true. Careful consideration is needed when applying Manhattan distance to datasets with varying importance of dimensions.</p> </li> <li> <p>Limited Discriminative Power: Compared to Euclidean distance, Manhattan distance may have limited discriminative power. It measures distance along axes rather than considering the direct line between two points. This may result in less accurate discrimination between points in certain scenarios.</p> </li> </ol> <p>Consider these advantages and disadvantages when deciding whether to use Manhattan distance in your specific machine learning tasks. It is essential to understand the characteristics and limitations of this distance metric and consider alternative distance metrics or data preprocessing techniques if necessary.</p>"},{"location":"Machine%20Learning/different-distance-calculations/#minkowski-distance","title":"Minkowski Distance","text":"<p>Minkowski distance is a generalized distance metric that includes both Euclidean and Manhattan distances. It allows you to control the distance calculation using a parameter called \"p.\" The formula for Minkowski distance between two points A and B in n-dimensional space is:</p> \\[\\large d(A, B) = (\\sum_{i=1}^{n}|A_i - B_i|^p)^(1/p)\\] <p>Python code for this technique is</p> <pre><code>import numpy as np\nfrom numpy.linalg import norm\n\ndef minkowski_distance(A, B, p):\n    return np.power(np.sum(np.power(np.abs(A - B), p)), 1 / p)\n\nA = np.array([1, 2, 3])\nB = np.array([4, 5, 6])\np = 3\n\ndistance = minkowski_distance(A, B, p)\nprint(\"Minkowski Distance:\", distance)\n</code></pre>"},{"location":"Machine%20Learning/different-distance-calculations/#advantages_2","title":"Advantages","text":"<ol> <li> <p>Generalization of Distance Metrics: Minkowski distance is a generalized distance metric that includes both Euclidean distance (when p = 2) and Manhattan distance (when p = 1). By adjusting the parameter p, you can control the behavior of the distance calculation, making it adaptable to different scenarios and data characteristics.</p> </li> <li> <p>Flexibility in Handling Different Data Types: Minkowski distance can handle a wide range of data types, including numerical, categorical, and ordinal data. By appropriately choosing the value of p, you can accommodate different data distributions and properties.</p> </li> <li> <p>Continuity: Minkowski distance is a continuous metric, meaning that small changes in the input data result in small changes in the calculated distance. This property is desirable in many applications where small variations in the data should reflect small variations in the distance measurement.</p> </li> </ol>"},{"location":"Machine%20Learning/different-distance-calculations/#disadvantages_2","title":"Disadvantages","text":"<ol> <li> <p>Sensitivity to Irrelevant Dimensions: Like Euclidean distance, Minkowski distance can be sensitive to irrelevant or noisy dimensions in the data. If certain dimensions are not informative or contain noise, they can impact the overall distance calculation and potentially lead to suboptimal results.</p> </li> <li> <p>Parameter Sensitivity: The choice of the parameter p in Minkowski distance can significantly impact the distance calculation. Different values of p may yield different results and interpretations. Selecting an appropriate value for p requires domain knowledge and experimentation.</p> </li> <li> <p>Curse of Dimensionality: Minkowski distance, like Euclidean distance, is affected by the curse of dimensionality. As the number of dimensions increases, the distances between points tend to become more similar, making it challenging to discriminate between different points accurately.</p> </li> <li> <p>Computational Complexity: The computational complexity of Minkowski distance increases with the dimensionality of the data. As the number of dimensions grows, the computation becomes more time-consuming and resource-intensive.</p> </li> </ol> <p>It's important to carefully consider the advantages and disadvantages of Minkowski distance in your specific problem domain. Understanding the characteristics and limitations of this distance metric will help you make informed decisions when applying it in machine learning algorithms, such as K-nearest neighbors or clustering algorithms. Experimenting with different parameter values and preprocessing techniques can help mitigate some of the disadvantages and optimize the distance calculation for your specific data.</p>"},{"location":"Machine%20Learning/different-distance-calculations/#cosine-similarity","title":"Cosine Similarity","text":"<p>Cosine similarity measures the cosine of the angle between two vectors, representing the similarity between their orientations. It is commonly used in text mining and recommendation systems. The formula for cosine similarity between two vectors A and B is:</p> \\[\\large Cosine \\ Similarity = \\frac{A.B}{|A||B|}\\] <p>The Python code should be</p> <pre><code>import numpy as np\nfrom numpy.linalg import norm\n\ndef cosine_similarity(A, B):\n    dot_product = np.dot(A, B)\n    norm_product = norm(A) * norm(B)\n    return dot_product / norm_product\n\nA = np.array([1, 2, 3])\nB = np.array([4, 5, 6])\n\nsimilarity = cosine_similarity(A, B)\nprint(\"Cosine Similarity:\", similarity)\n</code></pre>"},{"location":"Machine%20Learning/different-distance-calculations/#advantages_3","title":"Advantages","text":"<ol> <li> <p>Insensitive to Magnitude: Cosine similarity is insensitive to the magnitude or length of the vectors being compared. It focuses on the orientation of the vectors rather than their absolute values. This makes it suitable for comparing documents or text data where the magnitude of feature values may vary.</p> </li> <li> <p>Effective for High-Dimensional Data: Cosine similarity is particularly useful in high-dimensional spaces. In such spaces, the distances between points tend to become more similar, making traditional distance metrics less effective. Cosine similarity can capture the similarity between vectors based on their orientations, even in high-dimensional spaces.</p> </li> <li> <p>Robustness to Sparse Data: Cosine similarity performs well with sparse data, where most of the dimensions have zero values. It only considers the non-zero dimensions, effectively capturing the similarity between sparse vectors.</p> </li> <li> <p>Popular in Text Mining and Recommendation Systems: Cosine similarity is widely used in text mining, information retrieval, and recommendation systems. It helps measure the similarity between documents or items based on their textual content or feature vectors.</p> </li> </ol>"},{"location":"Machine%20Learning/different-distance-calculations/#disadvantages_3","title":"Disadvantages","text":"<ol> <li> <p>Insensitivity to Orthogonal Differences: Cosine similarity treats vectors with orthogonal differences as dissimilar, even if they may have meaningful differences in other dimensions. It solely focuses on the angle between vectors and disregards such orthogonal differences. In certain scenarios, this may lead to inaccurate similarity measurements.</p> </li> <li> <p>Lack of Contextual Information: Cosine similarity does not consider the contextual information or semantics of the data. It solely relies on the vector representations and their orientations. In some applications, capturing the contextual meaning of data may be essential, and cosine similarity may fall short in providing accurate similarity measures.</p> </li> <li> <p>Dependency on Vector Representations: Cosine similarity heavily relies on the vector representations of data. The quality of vectorization techniques and the choice of features can significantly impact the similarity results. If the vector representations are not well-designed or relevant features are not captured, the cosine similarity may produce suboptimal results.</p> </li> <li> <p>Biased towards Dense Vectors: Cosine similarity tends to favor dense vectors over sparse vectors. Dense vectors often have more non-zero dimensions, leading to potentially higher similarity scores. This bias can affect the similarity measurements, especially when comparing sparse and dense vectors.</p> </li> </ol> <p>Consider these advantages and disadvantages when deciding to use cosine similarity in your specific machine learning tasks. While cosine similarity offers valuable properties, it's important to understand its limitations and consider alternative similarity measures or techniques based on the nature of your data and the specific problem at hand.</p>"},{"location":"Machine%20Learning/different-distance-calculations/#conclusion","title":"Conclusion","text":"<p>In this blog, we explored several distance calculation techniques commonly used in machine learning. We covered the Euclidean distance, Manhattan distance, Minkowski distance, and cosine similarity. For each technique, we provided the mathematical formula and demonstrated its implementation using Python examples.</p> <p>Understanding these distance metrics is crucial for various machine learning tasks, such as clustering algorithms (e.g., K-means), classification algorithms (e.g., K-nearest neighbors), and recommendation systems. By utilizing the appropriate distance metric, you can measure the similarity or dissimilarity between data points and make informed decisions based on their proximity in the feature space.</p> <p>Remember, the choice of distance metric depends on the nature of your data and the specific problem at hand. Experimenting with different distance metrics can help you uncover patterns and relationships in your data, leading to more accurate and robust machine learning models.</p> <p>By mastering these distance calculation techniques, you will have a solid foundation for tackling a wide range of machine learning problems effectively.</p>"},{"location":"Statistics/One-Way-ANOVA/","title":"Understanding One Way ANOVA","text":"<p>In the realm of statistical analysis, ANOVA (Analysis of Variance) is a powerful tool that allows researchers to uncover significant differences between multiple groups or treatments. Whether you're conducting a scientific study, analyzing market research data, or simply curious about statistical methodologies, understanding ANOVA can provide valuable insights into the relationships and patterns within your data.</p>"},{"location":"Statistics/One-Way-ANOVA/#what-is-anova","title":"What is ANOVA?","text":"<p>ANOVA is a statistical technique that compares the means of two or more groups to determine if there are any significant differences among them. It assesses the variability within and between the groups to draw meaningful conclusions about the factors influencing the observed differences. ANOVA is particularly useful when you want to test the null hypothesis that there are no significant differences between the groups.</p>"},{"location":"Statistics/One-Way-ANOVA/#types-of-anova","title":"Types of ANOVA","text":"<p>There are various types of ANOVA, each suited for different scenarios. The most common types include one-way ANOVA, two-way ANOVA, and repeated measures ANOVA. One-way ANOVA analyzes the effects of a single factor, while two-way ANOVA examines the influence of two factors simultaneously. Repeated measures ANOVA is used when data is collected from the same subjects over multiple time points or conditions.</p> <pre><code>graph TD\n  A[ANOVA] --&gt; B{One Way ANOVA};\n  A --&gt; C{Two Way ANOVA};\n  A --&gt; D{Repeated Measures ANOVA};</code></pre>"},{"location":"Statistics/One-Way-ANOVA/#f-distribution","title":"F-Distribution","text":"<p>Before starting the ANOVA, we have to learn F-Distribution because this statistical tests are based on the F - Distribution. The F distribution is a probability distribution that arises in the context of statistical inference, particularly in analysis of variance (ANOVA) and regression analysis. It is named after the statistician Sir Ronald Fisher, who developed many fundamental concepts in statistics.</p> <p></p> <p>This is a continuous probability distribution by using the Chi-Square Distribution \\((\u03c7^2)\\) and every Chi-Square distribution has one one parameter - degrees of freedom (\\(df\\)).</p> <p>Lets assume, we have 2 Chi - Square Distribution \\((\u03c7^2_1)\\) &amp; \\((\u03c7^2_2)\\) and their degrees of freedoms are \\(df_1\\) and \\(df_2\\) respectively. Then</p> \\[F-Distribution = \\frac{\u03c7^2_1/df_1}{\u03c7^2_2/df_2}\\] <p>Because of the Chi-Square distribution takes only non-negative values, the F distribution doesn't have non-negative values. It is positively skewed and has different shapes depending on the degrees of freedom associated with it. This is commonly used to test hypothesis about the equality of two variances in different samples or populations. The F distribution is widely used in various fields of research, including psychology, education, economics and the natural and social sciences, for hypothesis testing and model comparison.</p> <p>The F-statistics is calculated by dividing the ratio of two sample variances or mean squares from an ANOVA table. This value is then compared to critical values from the F-distribution to determine statistical significance.</p>"},{"location":"Statistics/One-Way-ANOVA/#one-way-anova","title":"One Way ANOVA","text":"<p>One-way ANOVA (Analysis of Variance) is a statistical technique that allows researchers to compare the means of three or more groups to determine if there are any significant differences among them. This powerful tool is widely used in various fields, from social sciences and psychology to business and healthcare. Understanding the basics of one-way ANOVA can help unravel hidden patterns and group distinctions within your data.</p> <p>One-way ANOVA is a parametric statistical test that examines the variability between multiple groups based on a single independent variable, also known as a factor. The goal is to determine if the observed differences in means across groups are statistically significant or simply due to chance. This type of ANOVA is particularly useful when you want to compare more than two groups simultaneously.</p>"},{"location":"Statistics/One-Way-ANOVA/#working-principle-of-one-way-anova","title":"Working principle of One-way ANOVA","text":"<p>One-way ANOVA works by decomposing the total variability in the data into two components: the variability between groups and the variability within groups. It then calculates an F-statistic by comparing the ratio of between-group variability to within-group variability. The F-statistic follows an F distribution, which is used to assess the statistical significance of the group differences.</p>"},{"location":"Statistics/One-Way-ANOVA/#steps-to-calculate-one-way-anova","title":"Steps to calculate One Way ANOVA","text":"<ol> <li>Define the null (\\(H_0\\)) and alternative hypothesis (\\(H_1\\)). The null hypothesis is that all the group means are equal. The \\(H_1\\) is that at least one group is significantly different from others.</li> <li>Calculate the overall mean (grand mean) of all the groups combined and mean of all the groups individually.</li> <li>Calculate the \"between-group\" and \"within-group\" sum of squares (SS) and their respective degrees of freedom.</li> <li>Calculate the \"between-group\" and \"within-group\" mean squares (MS) by dividing their respective sum of the squares by their degrees of freedom.</li> <li>Calculate the F-Statistics by dividing the \"between-group\" mean square by the \"within-group\" mean square.</li> <li>By using the F-Statistics, calculate the P-Value.</li> <li>By comparing the P-value with your threshold (\\(\\alpha\\)), either accept or reject the Null Hypothesis.</li> </ol> <p>This calculation can be represented by The ANOVA Table</p> Source of variance Sums of Squares Degrees of freedom Mean square F-Statistics P-value Between groups \\(SS_{b}\\) k-1 \\(MS_{b}\\) \\(MS_b/MS_w\\) p Within groups \\(SS_{w}\\) N-k \\(MS_{w}\\) - - Total \\(SS_{t}\\) N-1 - - -"},{"location":"Statistics/One-Way-ANOVA/#hands-on-example","title":"Hands on Example","text":"<p>Let's see we have a dataset like the below one. We have 3 groups (A, B and C) and for each group has different numerical values.</p> <p> A B C 3 1 8 6 8 6 3 9 10 <p></p> <p>You can assume the \"Pclass\" and \"Age\" column from the famous titanic dataset. The \"Pclass\" has 3 categories e.g., class-1, class-2 and class-3 and the numericals are the \"Age\" column values. The \"Pclass\" is called factors and categories/groups are called levels. For every level can have different number of values. For each value of the levels are called individuals.</p> <ul> <li>Step-1: Define the \\(H_0\\) hypothesis.</li> </ul> \\[H_0 = \\mu_A =\\mu_B = \\mu_C\\] <p>Define the \\(H_1\\). \\(H_1 =\\) At-least one mean is significantly different.</p> <ul> <li>Step-2: Now, for every group we have to find the mean. So,</li> </ul> \\[\\overline{X_A} = 4; \\ \\overline{X_B} = 6; \\ \\overline{X_C} = 8\\] <p>And the grand mean is</p> \\[\\overline{X} = \\frac{\\overline{X_A} + \\overline{X_B} + \\overline{X_C}}{3} = 6\\] <ul> <li>Step-3: Now time to calculate the SS (Sum of Squares).</li> </ul> \\[\\large SS_{total} = \\sum_{j=1}^{levels}\\sum_{i=1}^{individuals}(\\overline{X_{ij}} - \\overline{X})^2 = 76\\] <p>and the degrees of freedom is</p> \\[\\large df_{total} = N - 1 = 9 - 1 = 8\\] <p>where, \\(N\\) is the total individuals.</p> <p>Now we will calculate the Sum of Squares Between.</p> <p>$$ \\large SS_{between} = \\sum_{j=1}^{levels}(\\overline{X_j} - \\overline{X})^2*n_j = 24 $$.</p> \\[ \\large df_{between} = k - 1 = 3 - 1 = 2 \\] <p>Where \\(k\\) is the number of levels.</p> <p>We can also calculate the Sum of Squares Within.</p> \\[ \\large SS_{within} =  \\sum_{j=1}^{levels}\\sum_{i=1}^{individuals}(X_{ij} - \\overline{X_j})^2 = 52 \\] \\[ \\large df_{within} = N - k = 9 - 3 = 6 \\] <p>You can notice that</p> \\[ \\large SS_{total} = SS_{between} + SS_{within} \\] \\[ \\large df_{total} = df_{between} + SS_{within} \\] <ul> <li>Step-4: Now time to calculate the Mean of Squares from the Sum of Squares.</li> </ul> \\[ \\large MS_{between} = \\frac{SS_{between}}{df_{between}} = \\frac{24}{2} = 12 \\] \\[ \\large MS_{within} = \\frac{SS_{within}}{df_{within}} = \\frac{52}{6} = 8.67 \\] <ul> <li>Step-5: Now time to calculate the F-Statistics.</li> </ul> \\[ \\large F_{k-1, N-k} = \\frac{MS_{between}}{MS_{within}} = \\frac{12}{8.67} = 1.384 \\] <ul> <li>Step-6: By using the F-Statistics value, we will calculate the P-Value. For that you can use the below python code.</li> </ul> <p><pre><code>import scipy.stats as stats\n\nf_statistic = 1.384  # The F-statistic value you've calculated\ndf1 = 2              # Degrees of freedom for the numerator (between groups)\ndf2 = 6              # Degrees of freedom for the denominator (within groups)\n\np_value = stats.f.sf(f_statistic, df1, df2)\nprint(\"P-value:\", p_value)\n</code></pre> And the value is calculated 0.320.</p> <ul> <li>Step-7: The P-value is much greater than the significant value (\\(\\alpha = 0.05\\)). So, we can't find enough evidence to reject the null hypothesis. That's why we can say that the groups/levels are taken from the same population.</li> </ul>"},{"location":"Statistics/One-Way-ANOVA/#assumptions-of-one-way-anova","title":"Assumptions of One Way ANOVA","text":"<ol> <li> <p>Independence: The observations within and between groups should be independent of each other. This means that the outcome of one observation should not influence the outcome of another. Independence is typically achieved through random sampling or random assignment of subjects to groups.</p> </li> <li> <p>Normality: The data within each group should be approximately normally distributed. While one-way ANOVA is considered to be robust to moderate violations of normality, severe deviations may affect the accuracy of the test results. If normality is in doubt, non-parametric alternatives like the Shapiro-Wilk test can be considered.</p> </li> <li> <p>Homogeneity of variances: The variances of the populations from which the samples are drawn should be equal, or at-least approximately so. This assumption is known as homoscedasticity. If the variances are substantially different, the accuracy of the test results may be compromised. Levene's test or Bartlett's test can be used to assess the homogeneity of variance. If this assumption is violated, alternative tests such as Welch's ANOVA can be used.</p> </li> </ol>"},{"location":"Statistics/One-Way-ANOVA/#practical-example-with-code","title":"Practical example with code","text":"<pre><code># import the necessary libraries\nimport seaborn as sns\nimport pingouin as pg  # pip install pingouin\n\n# load the famous titanic dataset\ndf = sns.load_dataset('titanic')\n\n# calculate the ANOVA table\nanova_table = pg.anova(data=df, dv='age', between='pclass', detailed=True)\n\nprint(anova_table)\n</code></pre> <p>The result of the above is like this:</p> Source SS DF MS F p-unc np2 pclass 20929.627754 2 10464.813877 57.443484 7.487984e-24 0.139107 Within 129527.008190 711 182.175820 NaN NaN NaN <p>The p-unc is the P-value of this ANOVA test. You can see that the P-value is very less compared to the significant level (\\(\\alpha = 0.05\\)). So we can get enough evidence to reject the Null Hypothesis (\\(H_0\\)). That's mean atleast one of the category of the pclass is significantly different from the others. But the ANOVA test can't tell which group(s) is/are significantly different from others. To know which group(s) is/are significantly different, we have to do Post-Hoc test. </p>"},{"location":"Statistics/One-Way-ANOVA/#post-hoc-test","title":"Post-Hoc Test","text":"<p>While ANOVA determines if there are significant differences between groups, it does not reveal the specific pairings of groups that differ. Post hoc tests address this limitation by allowing researchers to make multiple pairwise comparisons. These tests provide a more detailed understanding of the group differences, enabling researchers to draw more precise conclusions. Generally, it has two types:</p> <ol> <li>Bonferroni correction</li> <li>Tukey's HSD (Honestly Significant Differece) Test</li> </ol>"},{"location":"Statistics/One-Way-ANOVA/#bonferroni-correction","title":"Bonferroni correction","text":"<p>In this test, perform all the possible t-test and then compare the result. Or, you can do few number of tests and compare them. This method adjusts the significance level (\\(\\alpha\\)) by dividing it by the number of comparisons being made. It is a conservation The Bonferroni correction is a conservative approach that adjusts the significance level for each comparison to maintain the overall family-wise error rate. This method divides the desired alpha level by the number of pairwise comparisons. While effective in controlling Type I error, the Bonferroni correction may be overly stringent and increase the chance of Type II error.</p> <pre><code>import scipy.stats as stats\n\nfor class1, class2 in [(1,2), (2, 3), (3, 1)]:\n    print(f\"Class {class1} vs Class {class2}\")\n    print(stats.ttest_ind(df[df['pclass'] == class1]['age'].dropna(),\n                          df[df['pclass'] == class2]['age'].dropna()))\n    print()\n</code></pre> <p>And the result is like this</p> <pre><code>Class 1 vs Class 2\nTtest_indResult(statistic=5.485187676773201, pvalue=7.835568991415144e-08)\n\nClass 2 vs Class 3\nTtest_indResult(statistic=3.927800191020872, pvalue=9.715078600777852e-05)\n\nClass 3 vs Class 1\nTtest_indResult(statistic=-10.849122601201033, pvalue=6.134470007830625e-25)\n</code></pre> <p>If we see the result, then the Class 1 vs Class 2 test and Class 3 vs Clas 1 test results are extremly significant. So, the Class 1 of \"pclass\" is significantly different from the others. But if we take significant level \\(\\alpha = \\frac{0.05}{3} = 0.0167\\) then all the groups are significantly different from each other.</p>"},{"location":"Statistics/One-Way-ANOVA/#tukeys-hsd-test","title":"Tukey's HSD Test","text":"<p>Tukey's HSD test is widely used and provides simultaneous confidence intervals for all possible pairwise comparisons. It controls the family-wise error rate, making it a popular choice. Tukey's HSD test is suitable for equal and unequal sample sizes.</p> <pre><code># import the required libraries\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# select our columns and drop the NaN values\nrequired_df = df[['age', 'pclass']].dropna()\ntukey = pairwise_tukeyhsd(endog=required_df['age'], groups=required_df['pclass'], alpha=0.05)\n\ntukey.plot_simultaneous()\n\nplt.show()\n</code></pre> <p>The result of the above code is</p> <p></p> <p>You can also see the summary of this test by running the below code.</p> <pre><code>pd.DataFrame(tukey.summary())\n</code></pre> <p>The result is</p> <p></p>"},{"location":"Statistics/One-Way-ANOVA/#disadvantages-of-one-way-anova","title":"Disadvantages of One Way ANOVA","text":"<p>While one-way ANOVA is a useful statistical technique, it also has certain drawbacks that researchers should be aware of. Here are some common drawbacks of one-way ANOVA:</p> <p>Assumption of Homogeneity of Variances: One-way ANOVA assumes that the variances across all groups are equal. Violation of this assumption, known as heteroscedasticity, can affect the accuracy and validity of the results. It may lead to inflated or deflated Type I error rates, making it challenging to draw accurate conclusions.</p> <p>Sensitivity to Outliers: One-way ANOVA can be sensitive to outliers, particularly when the sample sizes are small. Outliers can significantly impact the mean and variance, potentially influencing the results and leading to incorrect interpretations.</p> <p>Assumption of Normality: One-way ANOVA assumes that the data within each group follows a normal distribution. If the assumption is violated, it can affect the validity of the results. However, one-way ANOVA is often robust to violations of normality, especially when the sample sizes are large.</p> <p>Limited to One Independent Variable: As the name suggests, one-way ANOVA is designed to analyze the effects of a single independent variable or factor on a dependent variable. It is not suitable for investigating the influence of multiple independent variables or complex relationships between variables. In such cases, other statistical techniques like factorial ANOVA or regression analysis may be more appropriate.</p> <p>Post-hoc Interpretation: While one-way ANOVA can identify significant group differences, it does not provide specific information about which groups differ from each other. Post-hoc tests are often necessary to make pairwise comparisons, but these tests increase the chances of Type I errors. Multiple post-hoc tests also require careful consideration of family-wise error rate control methods.</p> <p>Lack of Causal Inference: One-way ANOVA examines associations and differences between groups, but it does not establish causality. It only provides evidence for the existence of group differences, not the underlying causes or mechanisms responsible for those differences. Causal relationships may require additional experimental designs or more sophisticated statistical analyses.</p> <p>Sample Size Considerations: One-way ANOVA performs better with larger sample sizes. Small sample sizes can limit the statistical power of the analysis, making it difficult to detect significant group differences accurately. It is crucial to ensure an adequate sample size to increase the reliability and generalizability of the findings.</p>"},{"location":"Statistics/One-Way-ANOVA/#applications-of-anova","title":"Applications of ANOVA","text":"<p>Hyperparameter tuning: When selecting the best hyperparameters for a machine learning model, one-way ANOVA can be used to compare the performance of models with different hyperparameter settings. By treating each hyperparameter settings as a group, you can perform one-way ANOVA to determine if there are any significant differences in performance across the various settings.</p> <p>Feature selection: One-way ANOVA can be used as a univariate feature selection method to idenity features that are significantly associated with the target variable, especially when the target variable is categorical with more than two levels. In this context, the one-way ANOVA is performed for each feature, and features with low p-values are considered to be more relevant for prediction.</p> <p>Algorithm comparison: When comparing the performance of different machine learning algorithms, one-way ANOVA can be used to determine if there are any significant differences in their performance metrics (e.g., accuracy, F1 score, etc.) across multiple runs or cross-validation folds. This can help you decide which algorithm is the most suitable for a specific problem.</p> <p>Model stability assessment: One-way ANOVA can be used to assess the stability of a machine learning model by comparing its performance across different random seeds or initializations. If the model's performance varies significantly between different initializations, it may indicate that the model is unstable or highly sensitive to the choice of initial conditions.</p>"},{"location":"Statistics/One-Way-ANOVA/#why-t-test-is-not-used","title":"Why t-test is not used","text":""}]}