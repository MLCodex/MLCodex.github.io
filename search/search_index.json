{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre> <p> <p></p>"},{"location":"Machine%20Learning/cross-validation/","title":"Cross Validation","text":"<p>"},{"location":"Machine%20Learning/cross-validation/#understanding-cross-validation-a-key-concept-in-machine-learning","title":"Understanding Cross-Validation: A Key Concept in Machine Learning","text":"<p>In today's world of data-driven decision-making, machine learning has emerged as a powerful tool for solving complex problems. One crucial technique in the toolkit of any machine learning practitioner is cross-validation. This blog post aims to provide a comprehensive introduction to cross-validation, including its types, the steps involved, and its benefits.</p>"},{"location":"Machine%20Learning/cross-validation/#what-is-cross-validation","title":"What is Cross Validation?","text":"<p>In the realm of machine learning, the primary goal is to build models that perform well not just on the training data they've seen, but on unseen data as well. To evaluate this generalizability, we need a reliable method. This is where cross-validation comes into play.</p> <p>Cross-validation is a statistical method used to estimate the performance of machine learning models. It helps us understand how the results of a statistical analysis will generalize to an independent dataset. It is primarily used in settings where the goal is prediction and one wants to estimate how accurately a predictive model will perform in practice.</p> <p>It involves dividing the available data into multiple non-overlapping folds or subsets, using one of these folds as a validation set, and training the model on the remaining folds. This process is repeated multiple times, each time using a different fold as the validation set. Finally, the results from each validation step are averaged to produce a more robust estimate of the model\u2019s performance.</p> <p>The main purpose of cross validation is to prevent overfitting, which occurs when a model is trained too well on the training data and performs poorly on new, unseen data. By evaluating the model on multiple validation sets, cross validation provides a more realistic estimate of the model\u2019s generalization performance, i.e., its ability to perform well on new, unseen data.</p>"},{"location":"Machine%20Learning/cross-validation/#types-of-cross-validation","title":"Types of Cross Validation","text":"<p>Several forms of cross-validation are used in the field of machine learning, each with its own benefits and drawbacks. Some of the most common types include:</p>"},{"location":"Machine%20Learning/cross-validation/#k-fold-cross-validation","title":"K-Fold Cross Validation","text":"<p>K-fold cross validation is one of the most common and widely used cross validation techniques. It involves splitting the data into k equal-sized subsets or folds. One of these folds is used as the validation set, and the model is trained on the remaining k-1 folds. This process is repeated k times, each time using a different fold as the validation set. The performance of the model is then calculated by averaging the scores obtained on each validation set.</p> <p></p> <p>If you look to the above image, you have devided your whole dataset into 5 folds. For the first iteration, you pick up the fold-1 to validate the model and the rest folds are used for trainig. Then in for the second iteration, you pick up the fold-2 (you should not to pick the fold-1 as it is already in the previous iteration) as testing dataset and the rest of the folds are used for training. This iterations are done for k times.</p> <p>The advantage of k-fold cross validation is that it reduces the variance of the model performance estimate, as it uses all the data for both training and testing. The disadvantage is that it can be computationally expensive, especially for large datasets or complex models.</p> <p>The choice of k depends on various factors, such as the size of the data, the complexity of the model, and the desired trade-off between bias and variance. A common value for k is 10, but it can vary depending on the situation. Below is the example to do the Cross Validation by making 10 folds using Scikit Learn library.</p> <pre><code>from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.datasets import load_boston\n\n# Load Boston housing dataset\nX, y = load_boston(return_X_y=True)\n\n# Create linear regression model\nmodel = LinearRegression()\n\n# Perform 10-fold cross validation\nscores = cross_val_score(model, X, y, cv=10)\n\n# Print scores and mean score\nprint(scores)\nprint(scores.mean())\n</code></pre>"},{"location":"Machine%20Learning/cross-validation/#leave-one-out-cross-validation-loocv","title":"Leave One Out Cross Validation (LOOCV)","text":"<p>Leave-one-out cross validation (LOOCV) is a special case of k-fold cross validation where k equals the number of observations in the data. In other words, it involves leaving out one observation from the data as the validation set, and training the model on the remaining n-1 observations. This process is repeated n times, each time using a different observation as the validation set. The performance of the model is then calculated by averaging the scores obtained on each validation set.</p> <p></p> <p>The advantage of LOOCV is that it uses all the data for both training and testing, and it does not introduce any randomness in the data splitting. The disadvantage is that it can be very computationally expensive, especially for large datasets or complex models.</p> <p>LOOCV is usually recommended when the data is very small or scarce, and every observation is valuable for training and testing. The following code snippet shows how to perform LOOCV on a linear regression model using scikit-leave:</p> <pre><code>from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import LeaveOneOut\nfrom sklearn.datasets import load_boston\n\n# Load Boston housing dataset\nX, y = load_boston(return_X_y=True)\n\n# Create linear regression model\nmodel = LinearRegression()\n\n# Create leave-one-out splitter\nloo = LeaveOneOut()\n\n# Perform leave-one-out cross validation\nscores = []\nfor train_index, test_index in loo.split(X):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    model.fit(X_train, y_train)\n    score = model.score(X_test, y_test)\n    scores.append(score)\n\n# Print scores and mean score\nprint(scores)\nprint(sum(scores)/len(scores))\n</code></pre>"},{"location":"Machine%20Learning/cross-validation/#stratified-cross-validation","title":"Stratified Cross Validation","text":"<p>Stratified cross validation is a variation of k-fold cross validation that preserves the proportion of each class or category in the data. It is useful when the data is imbalanced, i.e., when some classes or categories are overrepresented or underrepresented in the data. In such cases, using regular k-fold cross validation can result in biased or inaccurate estimates of the model performance, as some folds may contain only or mostly examples of one class or category.</p> <p></p> <p>Stratified cross validation ensures that each fold has the same or similar distribution of classes or categories as the original data. This way, the model is trained and tested on more representative samples of the data.</p> <p>Stratified cross validation can be applied to both classification and regression problems, depending on whether the target variable is categorical or continuous. For classification problems, stratified k-fold cross validation can be performed using scikit-learn\u2019s <code>StratifiedKFold</code> class. For regression problems, stratified k-fold cross validation can be performed using scikit-learn\u2019s <code>StratifiedShuffleSplit</code> class.</p> <p>The following code snippet shows how to perform stratified 10-fold cross validation on a logistic regression model using scikit-learn:</p> <pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.datasets import load_breast_cancer\n\n# Load breast cancer dataset\nX, y = load_breast_cancer(return_X_y=True)\n\n# Create logistic regression model\nmodel = LogisticRegression()\n\n# Create stratified 10-fold splitter\nskf = StratifiedKFold(n_splits=10)\n\n# Perform stratified 10-fold cross validation\nscores = []\nfor train_index, test_index in skf.split(X, y):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    model.fit(X_train, y_train)\n    score = model.score(X_test, y_test)\n    scores.append(score)\n\n# Print scores and mean score\nprint(scores)\nprint(sum(scores)/len(scores))\n</code></pre>"},{"location":"Machine%20Learning/cross-validation/#is-it-making-our-task-easier","title":"is it making our task easier?","text":""},{"location":"Machine%20Learning/cross-validation/#common-steps-for-cv","title":"Common steps for CV","text":"<ol> <li>Data splitting: Divide the entire dataset into k equal, non-overlapping subsets or \"folds\". If the dataset has n samples, each fold should contain roughly n/k samples.</li> <li>Model training: For each unique group, take the group as a test data set and take the remaining groups as a training data set. Fit a model on the training set and evaluate it on the test set.</li> <li>Model evaluation: Retain the evaluation score and discard the model. Scores may include accuracy, F1-score, precision, recall, or others depending on the task at hand.</li> <li>Results aggregation: The result of cross-validation is often given as the mean of the model skill scores, and also the variance or standard deviation of the scores which can give an idea about the model's stability. For example, a low standard deviation suggests the model is stable, and a high standard deviation suggests the model's performance is sensitive to the specific folds used for training and validation.</li> <li>Model selection: If you're comparing different models, choose the one with the highest mean score (and, ideally, the lowest variance).</li> <li>Final model training: Once the model type and hyperparameters have been selected, use all the data to train the final model.</li> </ol>"},{"location":"Machine%20Learning/cross-validation/#benefits-of-cross-validation","title":"Benefits of Cross Validation","text":"<p>The beauty of cross-validation lies in its simplicity and effectiveness. Here are some reasons why cross-validation is widely used:</p> <ol> <li>Model performance: It provides a robust estimate of the model's performance on unseen data. By using different subsets for training and validation in each iteration, we essentially use all our data for both training and validation, just not at the same time.</li> <li>Bias Variance trade-off: It strikes a good balance between bias and variance. With k-fold cross-validation, we aren't using too small a part of our data for validation (which might lead to a high bias), and also not too large a part (which might lead to high variance).</li> <li>Overfitting prevention: It helps in identifying models that overfit the training data. Overfitting is when a model performs very well on training data but poorly on unseen data. Since cross-validation involves testing the model's performance on unseen data, overfit models tend to stand out due to their poor validation performance.</li> <li>Hyperparameter tuning: It helps in tuning the hyperparameters of the model. By using cross-validation, we can search for the best hyperparameters that give the best performance on the validation set.</li> <li>Model selection: Cross-validation can help in selecting the model which would perform the best on unseen data. By comparing the validation scores of different models, we can select the one that performs the best.</li> </ol>"},{"location":"Machine%20Learning/cross-validation/#what-is-the-final-model-after-doing-cv-for-prediction","title":"What is the final model after doing CV for prediction?","text":"<p>A common source of confusion in cross-validation is about the final model that should be used for predictions on new, unseen data. It's important to remember that the purpose of cross-validation is not to create the final model, but to assess how the model's results will generalize to an independent data set, and to tune any hyperparameters.</p> <p>When you're performing k-fold cross-validation, you are indeed training k different models on k different subsets of the data. But the objective is to average the performance of these models, not to pick one of them for your final model.</p> <p>After you have evaluated the cross-validation score and tuned hyperparameters, you usually train your model again with these optimal hyperparameters, but now using the entire dataset. This final model, trained on all available data, is the one you use for future predictions.</p> <p>This approach makes intuitive sense because you're using the most information (i.e., the entire dataset) to train your final model. The cross-validation process only provides guidance on how to make that final model as good as possible. It tells you how well your model should perform on unseen data and which hyperparameters should give you the best performance.</p>"},{"location":"Machine%20Learning/cross-validation/#conclusion","title":"Conclusion","text":"<p>In summary, cross-validation is a valuable tool in a machine learning practitioner's arsenal. It's simple yet effective, providing a robust estimate of a model's real-world performance. Moreover, it helps in avoiding overfitting, tuning hyperparameters, and model selection. Despite its computational cost, the value it provides in building reliable and robust machine learning models is undeniable. Whether you're a seasoned practitioner or just starting out in the field of machine learning, understanding and effectively using cross-validation is an essential skill to have.</p> <p>The journey doesn't stop here though! Stay tuned to learn more about other techniques to fine-tune your machine learning models and to ensure they're ready to tackle real-world challenges.</p>"},{"location":"Machine%20Learning/different-distance-calculations/","title":"Exploring Different Distance Calculation Techniques","text":"<p>Distance calculation is a fundamental operation in various machine learning algorithms, and it plays a crucial role in tasks such as clustering, classification, and nearest neighbor search. In this blog, we will explore several distance calculation techniques commonly used in machine learning and provide Python examples to demonstrate their implementation. We will also provide the mathematical formulas for each distance metric.</p>"},{"location":"Machine%20Learning/different-distance-calculations/#different-types-of-distances","title":"Different Types of Distances","text":""},{"location":"Machine%20Learning/different-distance-calculations/#euclidean-distance","title":"Euclidean Distance","text":"<p>Euclidean distance is one of the most widely used distance metrics, calculating the straight-line distance between two points in a Euclidean space. The formula for Euclidean distance between two points A and B in n-dimensional space is:</p> \\[\\large d(A, B) = \\sqrt{\\sum_{i=1}^{n}(A_i - B_i)^2}\\] <p>Python example:</p> <pre><code>import numpy as np\n\ndef euclidean_distance(A, B):\n    return np.sqrt(np.sum((A - B) ** 2))\n\nA = np.array([1, 2, 3])\nB = np.array([4, 5, 6])\n\ndistance = euclidean_distance(A, B)\nprint(\"Euclidean Distance:\", distance)\n</code></pre>"},{"location":"Machine%20Learning/different-distance-calculations/#advantages","title":"Advantages","text":"<ol> <li> <p>Intuitive Interpretation: Euclidean distance is easy to understand and interpret. It measures the straight-line distance between two points, which aligns with our intuitive notion of distance in Euclidean space.</p> </li> <li> <p>Simplicity: Euclidean distance is relatively simple to calculate and implement. It involves the square root of the sum of squared differences between corresponding coordinates, making it computationally efficient.</p> </li> <li> <p>Popular and Widely Used: Euclidean distance is one of the most popular distance metrics and is extensively used in various fields, including machine learning, data mining, and spatial analysis. Many algorithms and techniques are built upon or rely on Euclidean distance.</p> </li> </ol>"},{"location":"Machine%20Learning/different-distance-calculations/#disadvantages","title":"Disadvantages","text":"<ol> <li> <p>Sensitive to Outliers: Euclidean distance is sensitive to outliers or extreme values in the dataset. Outliers can greatly affect the overall distance calculation, potentially leading to biased results.</p> </li> <li> <p>Limited Applicability to High-Dimensional Data: Euclidean distance becomes less effective in high-dimensional spaces. This phenomenon is known as the \"curse of dimensionality.\" In high-dimensional spaces, the distances between points tend to become more similar, diminishing the discriminative power of Euclidean distance.</p> </li> <li> <p>Assumptions of Linearity: Euclidean distance assumes linearity between features or dimensions. It assumes that each dimension contributes equally to the overall distance calculation. However, in some cases, this assumption may not hold, and certain dimensions may be more important than others.</p> </li> <li> <p>Not Robust to Feature Scaling: Euclidean distance is sensitive to differences in feature scales. If the scales of different features are not similar, the distance calculation can be dominated by the features with larger scales, leading to biased results. Feature scaling or normalization is often required to address this issue.</p> </li> </ol> <p>It's important to consider these advantages and disadvantages when using Euclidean distance in your analysis. Depending on the nature of your data and the specific problem at hand, alternative distance metrics or data preprocessing techniques may be more appropriate.</p>"},{"location":"Machine%20Learning/different-distance-calculations/#manhattan-distance","title":"Manhattan Distance","text":"<p>Manhattan distance, also known as city block distance or L1 distance, calculates the distance between two points by summing the absolute differences of their coordinates. The formula for Manhattan distance between two points A and B in n-dimensional space is:</p> \\[\\large d(A, B) = \\sum_{i=1}^{n}|A_i - B_i|\\] <p>Python code for Manhattan distance</p> <pre><code>import numpy as np\n\ndef manhattan_distance(A, B):\n    return np.sum(np.abs(A - B))\n\nA = np.array([1, 2, 3])\nB = np.array([4, 5, 6])\n\ndistance = manhattan_distance(A, B)\nprint(\"Manhattan Distance:\", distance)\n</code></pre>"},{"location":"Machine%20Learning/different-distance-calculations/#advantages_1","title":"Advantages","text":"<ol> <li> <p>Robustness to Outliers: Manhattan distance is less sensitive to outliers compared to Euclidean distance. Since it calculates the sum of absolute differences between coordinates, outliers have a limited impact on the overall distance calculation.</p> </li> <li> <p>Applicability to High-Dimensional Data: Manhattan distance performs relatively well in high-dimensional spaces compared to Euclidean distance. It is less affected by the \"curse of dimensionality\" and can provide meaningful distance measurements even when dealing with data with a large number of dimensions.</p> </li> <li> <p>Feature Selection: Manhattan distance can be useful in feature selection tasks. By considering the Manhattan distance between a feature vector and the origin, you can identify the most important features based on their contribution to the overall distance.</p> </li> <li> <p>Computational Efficiency: Manhattan distance involves a simple calculation of the sum of absolute differences, making it computationally efficient. It requires fewer computations compared to Euclidean distance, especially when dealing with large datasets.</p> </li> </ol>"},{"location":"Machine%20Learning/different-distance-calculations/#disadvantages_1","title":"Disadvantages","text":"<ol> <li> <p>Limited Interpretability: The interpretation of Manhattan distance is not as intuitive as Euclidean distance. It measures the distance as the sum of absolute differences in coordinates, which may not align with our intuitive notion of distance in Euclidean space.</p> </li> <li> <p>Sensitivity to Feature Scaling: Manhattan distance is sensitive to differences in feature scales. If the scales of different features are not similar, the distance calculation can be dominated by the features with larger scales, leading to biased results. Feature scaling or normalization is often required to address this issue.</p> </li> <li> <p>Assumptions of Equal Importance: Manhattan distance assumes equal importance for each dimension or feature. However, in some cases, certain dimensions may be more important than others, and this assumption may not hold true. Careful consideration is needed when applying Manhattan distance to datasets with varying importance of dimensions.</p> </li> <li> <p>Limited Discriminative Power: Compared to Euclidean distance, Manhattan distance may have limited discriminative power. It measures distance along axes rather than considering the direct line between two points. This may result in less accurate discrimination between points in certain scenarios.</p> </li> </ol> <p>Consider these advantages and disadvantages when deciding whether to use Manhattan distance in your specific machine learning tasks. It is essential to understand the characteristics and limitations of this distance metric and consider alternative distance metrics or data preprocessing techniques if necessary.</p>"},{"location":"Machine%20Learning/different-distance-calculations/#minkowski-distance","title":"Minkowski Distance","text":"<p>Minkowski distance is a generalized distance metric that includes both Euclidean and Manhattan distances. It allows you to control the distance calculation using a parameter called \"p.\" The formula for Minkowski distance between two points A and B in n-dimensional space is:</p> \\[\\large d(A, B) = (\\sum_{i=1}^{n}|A_i - B_i|^p)^(1/p)\\] <p>Python code for this technique is</p> <pre><code>import numpy as np\nfrom numpy.linalg import norm\n\ndef minkowski_distance(A, B, p):\n    return np.power(np.sum(np.power(np.abs(A - B), p)), 1 / p)\n\nA = np.array([1, 2, 3])\nB = np.array([4, 5, 6])\np = 3\n\ndistance = minkowski_distance(A, B, p)\nprint(\"Minkowski Distance:\", distance)\n</code></pre>"},{"location":"Machine%20Learning/different-distance-calculations/#advantages_2","title":"Advantages","text":"<ol> <li> <p>Generalization of Distance Metrics: Minkowski distance is a generalized distance metric that includes both Euclidean distance (when p = 2) and Manhattan distance (when p = 1). By adjusting the parameter p, you can control the behavior of the distance calculation, making it adaptable to different scenarios and data characteristics.</p> </li> <li> <p>Flexibility in Handling Different Data Types: Minkowski distance can handle a wide range of data types, including numerical, categorical, and ordinal data. By appropriately choosing the value of p, you can accommodate different data distributions and properties.</p> </li> <li> <p>Continuity: Minkowski distance is a continuous metric, meaning that small changes in the input data result in small changes in the calculated distance. This property is desirable in many applications where small variations in the data should reflect small variations in the distance measurement.</p> </li> </ol>"},{"location":"Machine%20Learning/different-distance-calculations/#disadvantages_2","title":"Disadvantages","text":"<ol> <li> <p>Sensitivity to Irrelevant Dimensions: Like Euclidean distance, Minkowski distance can be sensitive to irrelevant or noisy dimensions in the data. If certain dimensions are not informative or contain noise, they can impact the overall distance calculation and potentially lead to suboptimal results.</p> </li> <li> <p>Parameter Sensitivity: The choice of the parameter p in Minkowski distance can significantly impact the distance calculation. Different values of p may yield different results and interpretations. Selecting an appropriate value for p requires domain knowledge and experimentation.</p> </li> <li> <p>Curse of Dimensionality: Minkowski distance, like Euclidean distance, is affected by the curse of dimensionality. As the number of dimensions increases, the distances between points tend to become more similar, making it challenging to discriminate between different points accurately.</p> </li> <li> <p>Computational Complexity: The computational complexity of Minkowski distance increases with the dimensionality of the data. As the number of dimensions grows, the computation becomes more time-consuming and resource-intensive.</p> </li> </ol> <p>It's important to carefully consider the advantages and disadvantages of Minkowski distance in your specific problem domain. Understanding the characteristics and limitations of this distance metric will help you make informed decisions when applying it in machine learning algorithms, such as K-nearest neighbors or clustering algorithms. Experimenting with different parameter values and preprocessing techniques can help mitigate some of the disadvantages and optimize the distance calculation for your specific data.</p>"},{"location":"Machine%20Learning/different-distance-calculations/#cosine-similarity","title":"Cosine Similarity","text":"<p>Cosine similarity measures the cosine of the angle between two vectors, representing the similarity between their orientations. It is commonly used in text mining and recommendation systems. The formula for cosine similarity between two vectors A and B is:</p> \\[\\large Cosine \\ Similarity = \\frac{A.B}{|A||B|}\\] <p>The Python code should be</p> <pre><code>import numpy as np\nfrom numpy.linalg import norm\n\ndef cosine_similarity(A, B):\n    dot_product = np.dot(A, B)\n    norm_product = norm(A) * norm(B)\n    return dot_product / norm_product\n\nA = np.array([1, 2, 3])\nB = np.array([4, 5, 6])\n\nsimilarity = cosine_similarity(A, B)\nprint(\"Cosine Similarity:\", similarity)\n</code></pre>"},{"location":"Machine%20Learning/different-distance-calculations/#advantages_3","title":"Advantages","text":"<ol> <li> <p>Insensitive to Magnitude: Cosine similarity is insensitive to the magnitude or length of the vectors being compared. It focuses on the orientation of the vectors rather than their absolute values. This makes it suitable for comparing documents or text data where the magnitude of feature values may vary.</p> </li> <li> <p>Effective for High-Dimensional Data: Cosine similarity is particularly useful in high-dimensional spaces. In such spaces, the distances between points tend to become more similar, making traditional distance metrics less effective. Cosine similarity can capture the similarity between vectors based on their orientations, even in high-dimensional spaces.</p> </li> <li> <p>Robustness to Sparse Data: Cosine similarity performs well with sparse data, where most of the dimensions have zero values. It only considers the non-zero dimensions, effectively capturing the similarity between sparse vectors.</p> </li> <li> <p>Popular in Text Mining and Recommendation Systems: Cosine similarity is widely used in text mining, information retrieval, and recommendation systems. It helps measure the similarity between documents or items based on their textual content or feature vectors.</p> </li> </ol>"},{"location":"Machine%20Learning/different-distance-calculations/#disadvantages_3","title":"Disadvantages","text":"<ol> <li> <p>Insensitivity to Orthogonal Differences: Cosine similarity treats vectors with orthogonal differences as dissimilar, even if they may have meaningful differences in other dimensions. It solely focuses on the angle between vectors and disregards such orthogonal differences. In certain scenarios, this may lead to inaccurate similarity measurements.</p> </li> <li> <p>Lack of Contextual Information: Cosine similarity does not consider the contextual information or semantics of the data. It solely relies on the vector representations and their orientations. In some applications, capturing the contextual meaning of data may be essential, and cosine similarity may fall short in providing accurate similarity measures.</p> </li> <li> <p>Dependency on Vector Representations: Cosine similarity heavily relies on the vector representations of data. The quality of vectorization techniques and the choice of features can significantly impact the similarity results. If the vector representations are not well-designed or relevant features are not captured, the cosine similarity may produce suboptimal results.</p> </li> <li> <p>Biased towards Dense Vectors: Cosine similarity tends to favor dense vectors over sparse vectors. Dense vectors often have more non-zero dimensions, leading to potentially higher similarity scores. This bias can affect the similarity measurements, especially when comparing sparse and dense vectors.</p> </li> </ol> <p>Consider these advantages and disadvantages when deciding to use cosine similarity in your specific machine learning tasks. While cosine similarity offers valuable properties, it's important to understand its limitations and consider alternative similarity measures or techniques based on the nature of your data and the specific problem at hand.</p>"},{"location":"Machine%20Learning/different-distance-calculations/#conclusion","title":"Conclusion","text":"<p>In this blog, we explored several distance calculation techniques commonly used in machine learning. We covered the Euclidean distance, Manhattan distance, Minkowski distance, and cosine similarity. For each technique, we provided the mathematical formula and demonstrated its implementation using Python examples.</p> <p>Understanding these distance metrics is crucial for various machine learning tasks, such as clustering algorithms (e.g., K-means), classification algorithms (e.g., K-nearest neighbors), and recommendation systems. By utilizing the appropriate distance metric, you can measure the similarity or dissimilarity between data points and make informed decisions based on their proximity in the feature space.</p> <p>Remember, the choice of distance metric depends on the nature of your data and the specific problem at hand. Experimenting with different distance metrics can help you uncover patterns and relationships in your data, leading to more accurate and robust machine learning models.</p> <p>By mastering these distance calculation techniques, you will have a solid foundation for tackling a wide range of machine learning problems effectively.</p>"},{"location":"Statistics/Matrix/","title":"Basic guide of Matrix Algebra","text":"<p>"},{"location":"Statistics/Matrix/#inside-the-matrix-the-art-and-science-of-mathematical-marvels","title":"Inside the Matrix: The Art and Science of Mathematical Marvels","text":""},{"location":"Statistics/Matrix/#what-is-matrix","title":"What is Matrix?","text":"<p>Matrices, the plural form of a matrix, are the arrangements of numbers, variables, symbols, or expressions in a rectangular table that contains various numbers of rows and columns. They are rectangular-shaped arrays, for which different operations like addition, multiplication, and transposition are defined. The numbers or entries in the matrix are known as its elements. Horizontal entries of matrices are called rows and vertical entries are known as columns.</p>"},{"location":"Statistics/Matrix/#different-types-of-matrices","title":"Different types of Matrices","text":"<p>Before going to the mathematical operations with matrices, let's first see some types of it.</p>"},{"location":"Statistics/Matrix/#lower-upper-triangular-matrix","title":"Lower &amp; Upper Triangular Matrix","text":"<p>A lower triangular matrix is a square matrix whose all elements above the principal diagonal are zeros. A upper triangular matrix is a square matrix whose all elements below the principal diagonal are zeros. For example, the matrix</p> \\[ \\begin{bmatrix} 2 &amp; -1 &amp; 3\\\\ 0 &amp; 5 &amp; 2\\\\ 0 &amp; 0 &amp; -2 \\end{bmatrix} \\] <p>is an upper triangular matrix, and the matrix</p> \\[ \\begin{bmatrix} 2 &amp; 0 &amp; 0\\\\ 1 &amp; 5 &amp; 0\\\\ 1 &amp; -1 &amp; -2 \\end{bmatrix} \\] <p>is a lower triangular matrix.</p>"},{"location":"Statistics/Matrix/#diagonal-matrix","title":"Diagonal Matrix","text":"<p>A diagonal matrix is a matrix that is both upper triangular and lower triangular. i.e., all the elements above and below the principal diagonal are zeros and hence the name \"diagonal matrix\". Its mathematical definition is, a matrix A = [\\(a_{ij}\\)] is said to be diagonal if</p> <ul> <li>A is a square matrix</li> <li>\\(a_{ij} = 0\\) when \\(i \\neq j\\)</li> </ul> \\[ \\begin{bmatrix} 4 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; -2 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 7 \\end{bmatrix} \\] <p>Note to remember is that the diagonal elements of a diagonal matrix can be either zeros or non-zeros.</p> <ul> <li>Power: The nth power of a diagonal matrix (where n is a non-negative integer) can be obtained by raising each diagonal element to the power of n.</li> <li>Eigen values: The eigen values of a diagonal matrix are just the values on the diagonal. The corresponding eigen vectors are the standard basis vectors.\\</li> <li>Multiplication by a Vector: When a diagonal matrix multiplies a vector, it scales each component of the vector by the corresponding element on the diagonal.</li> <li>Matrix Multiplication: The product of two diagonal matrices is just the diagonal matrix with the corresponding elements on the diagonals multiplied.</li> </ul> <p>You can read more about the diagonal matrix here.</p>"},{"location":"Statistics/Matrix/#orthogonal-matrix","title":"Orthogonal Matrix","text":"<p>Let's say you have a matrix \\(A\\). You have calculated it's inverse (\\(A^{-1}\\)) and transpose (\\(A^T\\)) and if the formula \\(A^T = A^{-1}\\) satisfies, then the matrix \\(A\\) is called Orthogonal Matrix. So, you can rearrange the formula like this</p> <p>\\(A^T = A^{-1}\\)</p> <p>\\(=&gt; AA^T = AA^{-1} \\ (Multiply \\ A \\  to \\ the \\ both \\ sides)\\)</p> <p>\\(=&gt; AA^T = I \\ (Where \\ AA^{-1} \\ is \\ an \\ identical \\ matrix)\\)</p> <p>\\(=&gt; A^TA = I \\ (In \\ a \\ similar \\ way)\\)</p> <p>\\(=&gt; AA^T = A^TA = I\\)</p> <ul> <li>An orthogonal matrix is a square matrix whose columns and rows are orthogonal unit vectors (i.e., orthogonal vectors), meaning that they are all of unit length and are at right angles to each other.</li> <li>Inverse Equals Transpose: The transpose of an orthogonal matrix equals its inverse, e.e., \\(A^T = A^{-1}\\). This property makes calculations with orthogonal matrices computationally efficient.</li> </ul> <p>To know more, click here and for Identical Matrix.</p>"},{"location":"Statistics/Matrix/#symmetric-matrix","title":"Symmetric Matrix","text":"<p>A symmetric matrix is those matrix which satisfy the formula</p> \\[ A = A^T \\] <p>where \\(A\\) is a \\((n \\times n)\\) matrix.</p> <ul> <li>Real Eigen Values: The eigen values of a real symmetric matrix are always real, not complex.</li> <li>Orthogonal Eigen Vectors: For a real symmetric matrix, the eigen vectors corresponding to different eigen values are always orthogonal to each other. If the eigen values are distinct, you can even choose an orthonormal basis of eigen vectors.</li> </ul>"},{"location":"Statistics/Matrix/#determinate","title":"Determinate","text":"<p>The determinant of a matrix is a scalar value that is a function of the entries of a square matrix. It characterizes some properties of the matrix and the linear map represented by the matrix. In particular, the determinant is nonzero if and only if the matrix is invertible and the linear map represented by the matrix is an isomorphism.</p> <p>The determinant of a matrix \\(A\\) is represented by two vertical lines or simply by writing det and writing the matrix name. For example, \\(|A|\\), \\(det(A)\\), \\(det \\ A\\), or \\(\u0394\\).</p> <p>Calculate a 2X2 matrix determinate</p> <p>If we have a matrix</p> <p>\\(A = \\begin{bmatrix} a &amp; b\\\\ c &amp; d\\\\ \\end{bmatrix} \\ then \\ |A| = (a \\times d) - (b \\times c)\\)</p> <p>Calculate a 3X3 matrix determinate</p> <p>\\(A = \\begin{bmatrix} a_1 &amp; b_1 &amp; c_1\\\\ a_2 &amp; b_2 &amp; c_2\\\\ a_3 &amp; b_3 &amp; c_3\\\\ \\end{bmatrix} \\ then \\ |A| =  a_1.\\begin{vmatrix} b_2 &amp; c_2\\\\ b_3 &amp; c_3 \\end{vmatrix} -  b_1.\\begin{vmatrix} a_2 &amp; c_2\\\\ a_3 &amp; c_3 \\end{vmatrix} +  c_1.\\begin{vmatrix} a_2 &amp; b_2\\\\ a_3 &amp; b_3 \\end{vmatrix}\\)</p> <p>Calculate a 4X4 matrix determinate</p> <p>\\(A = \\begin{bmatrix} a_1 &amp; b_1 &amp; c_2 &amp; d_1\\\\ a_2 &amp; b_2 &amp; c_2 &amp; d_2\\\\ a_3 &amp; b_3 &amp; c_3 &amp; d_3\\\\ a_4 &amp; b_4 &amp; c_4 &amp; d_4 \\end{bmatrix}\\)</p> <p>\\(|A| =  a_1.\\begin{vmatrix} b_2 &amp; c_2 &amp; d_2\\\\ b_3 &amp; c_3 &amp; d_3\\\\ b_4 &amp; c_4 &amp; d_4 \\end{vmatrix} -  b_1.\\begin{vmatrix} a_2 &amp; c_2 &amp; d_2\\\\ a_3 &amp; c_3 &amp; d_3\\\\ a_4 &amp; c_4 &amp; d_4 \\end{vmatrix} + c_1.\\begin{vmatrix} a_2 &amp; b_2 &amp; d_2\\\\ a_3 &amp; b_3 &amp; d_3\\\\ a_4 &amp; b_4 &amp; d_4 \\end{vmatrix} - d_1.\\begin{vmatrix} a_2 &amp; b_2 &amp; c_2\\\\ a_3 &amp; b_3 &amp; c_3\\\\ a_4 &amp; b_4 &amp; c_4 \\end{vmatrix}\\)</p> <p>See the sign of the every even term while calculating the determinate. That'a an important term to remember.</p>"},{"location":"Statistics/Matrix/#properties","title":"Properties","text":"<ul> <li>The determinate of any 1X1 matrix is always equal to the element of the matrix.</li> <li>The determinate of a matrix is equal to the determinate of its transpose.</li> <li>If any two rows (or columns) of a determinant are interchanged, then the sign of the determinant changes.</li> <li>If any two rows (or columns) of a determinant are identical, then the determinant is 0.</li> <li>If all elements of a row (or column) of a matrix of a determinant are zeros, then the value of the determinant is 0.</li> <li>If each element of a row (or column) of a determinant is multiplied by a scalar k, then the value of the resultant determinant is k times the value of the original determinant.</li> <li>If each element of a row (or column) of a determinant is expressed as sum of two (or more) numbers, then the determinant can be split into the sum of two (or more) determinants.</li> <li>If each element of a row (or column) is multiplied by a constant and the elements are added to the corresponding elements of another row (or column), then the determinant remains unchanged.</li> <li>The determinant of an identity matrix is always 1.</li> <li>The determinant of a diagonal matrix is always the product of elements of its principal diagonal.</li> <li>The determinant of an orthogonal matrix is either +1 or -1.</li> </ul>"},{"location":"Statistics/Matrix/#inverse-matrix","title":"Inverse Matrix","text":"<p>The inverse of a matrix is a matrix that, when multiplied by the original matrix, gives the identity matrix. The inverse of a matrix \\(A\\) is denoted by \\(A^{-1}\\), and it satisfies the property</p> AA\u22121=A\u22121A=IAA^{-1} = A^{-1}A = I  <p>Not every matrix has an inverse. Only square matrices can have an inverse, and only if their determinant is not zero. The determinant is a scalar value that can be calculated from a square matrix and has some properties related to the linear transformation represented by the matrix. A matrix that has an inverse is called invertible or nonsingular, and a matrix that does not have an inverse is called noninvertible or singular.</p>"},{"location":"Statistics/Matrix/#how-to-find-the-inverse-of-a-matrix","title":"How to find the inverse of a matrix?","text":"<p>There are different methods to find the inverse of a matrix, depending on the size and complexity of the matrix. Some of the common methods are:</p> <ul> <li>For a 2 x 2 matrix, the inverse can be found using a simple formula:</li> </ul> [abcd]\u22121=1ad\u2212bc[d\u2212b\u2212ca]\\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix}^{-1} = \\frac{1}{ad-bc} \\begin{bmatrix} d &amp; -b \\\\ -c &amp; a \\end{bmatrix}  <p>where \\(ad-bc\\) is the determinant of the matrix.</p> <ul> <li>For a larger matrix (n x n), the inverse can be found using the adjoint method. This method involves finding the minors and cofactors of the elements of the matrix, and then using them to form the adjugate or adjoint matrix. The adjugate matrix is the transpose of the cofactor matrix. The cofactor matrix is obtained by multiplying each minor by (-1)^(i+j), where i and j are the row and column indices of the element. The minor of an element is the determinant of the submatrix obtained by deleting its row and column. The formula for the inverse using this method is:</li> </ul> A\u22121=1\u2223A\u2223adj(A)A^{-1} = \\frac{1}{|A|}\\text{adj}(A)  <p>where \\(|A|\\) is the determinant of \\(A\\) and \\(adj(A)\\) is the adjugate of A. For example, if</p> \\[ A = \\begin{bmatrix} 1 &amp; 2 &amp; 3\\\\ 0 &amp; -4 &amp; 1\\\\ 2 &amp; 0 &amp; -3 \\end{bmatrix} \\ then \\ |A| = 48 \\] <p>The minors of A are:</p> \\[M_{11} = \\begin{bmatrix} -4 &amp; 1\\\\ 0 &amp; -3 \\end{bmatrix} = 12\\] \\[M_{12} = \\begin{bmatrix} 0 &amp; 1\\\\ 2 &amp; -3 \\end{bmatrix} = -2\\] \\[M_{13} = \\begin{bmatrix} 0 &amp; -4\\\\ 2 &amp; 0 \\end{bmatrix} = 8\\] \\[M_{21} = \\begin{bmatrix} 2 &amp; 3\\\\ 0 &amp; -3 \\end{bmatrix} = -6\\] \\[M_{22} = \\begin{bmatrix} 1 &amp; 3\\\\ 2 &amp; -3 \\end{bmatrix} = -9\\] \\[M_{23} = \\begin{bmatrix} 1 &amp; 2\\\\ 2 &amp; 0 \\end{bmatrix} = -4\\] \\[M_{31} \u200b= \\begin{bmatrix} \u200b2 &amp; 3\\\\ \u200b-4 &amp; 1 \\end{bmatrix} = 14\\] \\[M_{32} \u200b= \\begin{bmatrix} \u200b1 &amp; 3\\\\ \u200b0 &amp; 1 \\end{bmatrix} \u200b\u200b= 1\\] \\[M_{33}\u200b = \\begin{bmatrix} \u200b1 &amp; 2\\\\ \u200b0 &amp; \u22124 \\end{bmatrix} \u200b\u200b= \u22124\\] <p>The cofactors of A are:</p> C11=(\u22121)1+1M11=12C_{11} = (-1)^{1+1}M_{11} = 12  C12=(\u22121)1+2M12=2C_{12} = (-1)^{1+2}M_{12} = 2  C13=(\u22121)1+3M13=8C_{13} = (-1)^{1+3}M_{13} = 8  C21=(\u22121)2+1M21=6C_{21} = (-1)^{2+1}M_{21} = 6  C22=(\u22121)2+2M22=\u22129C_{22} = (-1)^{2+2}M_{22} = -9  C23=(\u22121)2+3M23=4C_{23} = (-1)^{2+3}M_{23} = 4  C31=(\u22121)3+1M31=14C_{31} = (-1)^{3+1}M_{31} = 14  C32=(\u22121)3+2M32=\u22121C_{32} = (-1)^{3+2}M_{32} = -1  C33=(\u22121)3+3M33=\u22124C_{33} = (-1)^{3+3}M_{33} = -4  <p>The adjugate of A is:</p> adj(A)=[C11C21C31C12C22C32C13C23C33]=[126142\u22129\u2212184\u22124]\\text{adj}(A) = \\begin{bmatrix} C_{11} &amp; C_{21} &amp; C_{31}\\\\ C_{12} &amp; C_{22} &amp; C_{32}\\\\ C_{13} &amp; C_{23} &amp; C_{33}\\\\ \\end{bmatrix} = \\begin{bmatrix} 12 &amp; 6 &amp; 14\\\\ 2 &amp; -9 &amp; -1\\\\ 8 &amp; 4 &amp; -4\\\\ \\end{bmatrix}  <p>The inverse of A is:</p> A\u22121=1\u2223A\u2223adj(A)=148[126142\u22129\u2212184\u22124]=[0.250.1250.2920.042\u22120.188\u22120.0210.1670.083\u22120.083]A^{-1} = \\frac{1}{|A|}\\text{adj}(A) = \\frac{1}{48} \\begin{bmatrix} 12 &amp; 6 &amp; 14\\\\ 2 &amp; -9 &amp; -1\\\\ 8 &amp; 4 &amp; -4\\\\ \\end{bmatrix} = \\begin{bmatrix} 0.25 &amp; 0.125 &amp; 0.292\\\\ 0.042 &amp; -0.188 &amp; -0.021\\\\ 0.167 &amp; 0.083 &amp; -0.083\\\\ \\end{bmatrix}  <ul> <li>For a large or complex matrix, the inverse can be found using a numerical method such as Gaussian elimination or Newton\u2019s method. These methods involve performing a series of operations on the matrix until it is reduced to a simpler form, such as the identity matrix or a diagonal matrix. Then, the inverse can be obtained by applying the same operations to the identity matrix or solving a system of equations. These methods are usually implemented by computers or calculators.</li> </ul>"},{"location":"Statistics/Matrix/#why-is-the-inverse-of-a-matrix-important","title":"Why is the inverse of a matrix important?","text":"<p>The inverse of a matrix has many applications in mathematics and other fields. Some of them are:</p> <ul> <li>The inverse of a matrix can be used to solve systems of linear equations. For example, if \\(AX = B\\) is a system of n linear equations in n unknowns, where \\(A\\) is an invertible n x n matrix and X and B are n x 1 column vectors, then X can be found by multiplying both sides by \\(A^{-1}\\)</li> </ul> AX=B \u27f9 A\u22121(AX)=A\u22121(B) \u27f9 (A\u22121A)X=A\u22121(B) \u27f9 IX=A\u22121(B) \u27f9 X=A\u22121(B)AX = B \\implies A^{-1}(AX) = A^{-1}(B) \\implies (A^{-1}A)X = A^{-1}(B) \\implies IX = A^{-1}(B) \\implies X = A^{-1}(B)  <ul> <li>The inverse of a matrix can be used to find the inverse of a linear transformation. A linear transformation is a function that maps vectors to vectors and preserves addition and scalar multiplication.</li> </ul>"},{"location":"Statistics/Matrix/#eigen-vectors-eigen-values-in-terms-of-linear-transformation","title":"Eigen Vectors &amp; Eigen Values in terms of Linear Transformation","text":""},{"location":"Statistics/Matrix/#linear-transformation","title":"Linear Transformation","text":""},{"location":"Statistics/Matrix/#what-is-the-meaning-of-rn-in-coordinate-system","title":"What is the meaning of \\(R^n\\) in coordinate system?","text":"<p>The mean of coordinate system for \\(R^n\\) is the way of locating points or vectors in an n-dimensional space using n numbers. For example, a coordinate system for \\(R^2\\) is a way of locating points or vectors in a plane using two numbers, usually called x and y.</p> <p>There are different types of coordinate systems for \\(R^n\\), such as Cartesian, polar, cylindrical, spherical, etc. Each type has its own advantages and disadvantages depending on the context and the problem. The most common type is the Cartesian coordinate system, which uses n perpendicular axes (called x-axis, y-axis, z-axis, etc.) that intersect at a point called the origin. Any point or vector in \\(R^n\\) can be represented by an n-tuple of numbers (called coordinates) that indicate how far to move along each axis from the origin.</p> <p>For example, in a Cartesian coordinate system for \\(R^2\\), the point (3, 4) means moving 3 units along the x-axis and 4 units along the y-axis from the origin. The vector [2, -1] means moving 2 units along the x-axis and -1 unit along the y-axis from any point.</p> <p>A coordinate system for \\(R^n\\) also defines a basis for \\(R^n\\), which is a set of n linearly independent vectors that span the space. A basis can be used to write any vector in \\(R^n\\) as a linear combination of the basis vectors. For example, in a Cartesian coordinate system for \\(R^2\\), the standard basis is {e1, e2}, where e1 = [1, 0] and e2 = [0, 1]. Any vector [x, y] in \\(R^2\\) can be written as xe1 + ye2.</p> <p>A coordinate system for \\(R^n\\) can also be used to perform operations on points or vectors, such as addition, subtraction, scalar multiplication, dot product, cross product, etc. For example, in a Cartesian coordinate system for \\(R^3\\), the dot product of two vectors [\\(x_1, y_1, z_1\\)] and [\\(x_2, y_2, z_2\\)] is \\(x_1x_2 + y_1y_2 + z_1z_2\\). The cross product of two vectors [\\(x_1, y_1, z_1\\)] and \\([x_2, y_2, z_2]\\) is \\([y_1z_2 - z_1y_2, z_1x_2 - x_1z_2, x_1y_2 - y_1x_2]\\).</p> <p>A coordinate system for \\(R^n\\) can also be used to visualize geometric objects or transformations in \\(R^n\\). For example, in a Cartesian coordinate system for \\(R^3\\), a plane can be represented by an equation of the form ax + by + cz = d. A rotation can be represented by a matrix that multiplies each vector by a certain angle.</p>"},{"location":"Statistics/Matrix/#what-is-standard-basis-vectors","title":"What is standard basis vectors?","text":"<p>Standard basis vectors are vectors that have one entry equal to 1 and the rest equal to 0. They are used to represent the directions along the axes of a coordinate system. For example, in a two-dimensional space, the standard basis vectors are e1 = (1, 0) and e2 = (0, 1). They point along the x-axis and y-axis respectively. In a three-dimensional space, the standard basis vectors are e1 = (1, 0, 0), e2 = (0, 1, 0), and e3 = (0, 0, 1). They point along the x-axis, y-axis, and z-axis respectively.</p> <p>Standard basis vectors are useful because they can be used to write any vector as a linear combination of them. For example, in a two-dimensional space, any vector v = (x, y) can be written as v = xe1 + ye2. This means that the components of v are the coefficients of the standard basis vectors. Similarly, in a three-dimensional space, any vector v = (x, y, z) can be written as v = xe1 + ye2 + z*e3.</p> <p>Standard basis vectors are also orthogonal to each other, meaning that their dot product is zero. For example, e1 \u00b7 e2 = (1, 0) \u00b7 (0, 1) = 0. This means that they are perpendicular to each other and form right angles. Standard basis vectors are also unit vectors, meaning that their length or norm is one. For example, ||e1|| = \u221a(1^2 + 0^2) = 1. This means that they have the same size and only differ in direction.</p> <p>Standard basis vectors are sometimes denoted by i, j, k instead of e1, e2, e3. For example, i = (1, 0), j = (0, 1), k = (0, 0, 1). They are also sometimes written with a hat (^) over them to emphasize that they are unit vectors. For example, \u00ea1 = (1, 0), \u00ea2 = (0, 1), \u00ea3 = (0, 0, 1).</p>"},{"location":"Statistics/Matrix/#what-is-domain-and-codomain","title":"What is domain and codomain?","text":"<p>Domain and codomain are terms related to functions. A function is a rule that assigns an output to every input. For example, f(x) = x + 2 is a function that adds 2 to any input x.</p> <p>The domain of a function is the set of all possible inputs that the function can accept. For example, the domain of f(x) = x + 2 is the set of all real numbers, because we can add 2 to any real number.</p> <p>The codomain of a function is the set of all possible outputs that the function can produce. For example, the codomain of f(x) = x + 2 is also the set of all real numbers, because adding 2 to any real number gives another real number.</p> <p>The range of a function is the subset of the codomain that actually occurs as outputs when the function is applied to the domain. For example, the range of f(x) = x + 2 is the set of all real numbers greater than 2, because adding 2 to any real number gives a number greater than 2.</p> <p>The domain and codomain are part of the definition of a function, and they specify what kind of values the function can work with. The range is determined by how the function behaves on the domain, and it tells us what kind of values the function actually produces.</p>"},{"location":"Statistics/Matrix/#what-is-linear-transformation","title":"What is Linear Transformation?","text":"<p>A linear transformation is a function that maps vectors to vectors and preserves addition and scalar multiplication. For example, if \\(T\\) is a linear transformation, then  $$ T(\\vec{u} + \\vec{v}) = T(\\vec{u}) + T(\\vec{v}) $$ and $$ T(c\\vec{u}) = cT(\\vec{u}) $$ for any vectors \\(\\vec{u}\\) and \\(\\vec{v}\\) and any scalar c.</p> <p>A matrix is a rectangular array of numbers that can be used to perform a linear transformation by multiplying it with a vector. For example, if \\(A\\) is an \\(m \\times n\\) matrix and \\(\\vec{x}\\) is an \\(n \\times 1\\) vector, then \\(A\\vec{x}\\) is an \\(m \\times 1\\) vector that is the result of applying the linear transformation determined by A to \\(\\vec{x}\\).</p> <p>The matrix representation of a linear transformation depends on the choice of basis for the domain and the codomain. A basis is a set of linearly independent vectors that span the vector space. For example, the standard basis for \\(R^n\\) is the set of n vectors that have 1 in one entry and 0 in all other entries. Any vector in \\(R^n\\) can be written as a linear combination of these basis vectors.</p> <p>To find the matrix representation of a linear transformation \\(T: R^n \u2192 R^m\\) with respect to the standard basis, we need to apply \\(T\\) to each basis vector of \\(R^n\\) and write the result as a column vector in \\(R^m\\). Then, we arrange these column vectors as the columns of an \\(m \\times n\\) matrix A. This matrix A satisfies the equation:</p> \\[ T(\\vec{x}) = A\\vec{x} \\] <p>for any vector \\(\\vec{x}\\) in \\(R^n\\).</p>"},{"location":"Statistics/Matrix/#how-to-visualize-the-linear-transformation","title":"How to visualize the Linear Transformation?","text":"<p>To visualize a linear transformation using matrix, we can use the following steps:</p> <ul> <li>Draw a coordinate system for \\(R^n\\) and plot the standard basis vectors e1, e2, \u2026, en as arrows from the origin.</li> <li>Draw another coordinate system for \\(R^m\\) and plot the columns of A as arrows from the origin. These are the images of the standard basis vectors under \\(T\\).</li> <li>To find the image of any vector \\(\\vec{x}\\) in \\(R^n\\) under \\(T\\), write \\(\\vec{x}\\) as a linear combination of the standard basis vectors: \\(\\vec{x} = x1e1 + x2e2 + \u2026 + xnen\\). Then, use the distributive property of matrix multiplication to write: \\(T(\\vec{x}) = A\\vec{x} = Ax1e1 + Ax2e2 + \u2026 + Axnen\\). This means that \\(T(\\vec{x})\\) is also a linear combination of the columns of \\(A: T(\\vec{x}) = x1Ae1 + x2Ae2 + \u2026 + xnAen\\). Therefore, we can plot \\(T(\\vec{x})\\) in \\(R^m\\) by adding up the scaled columns of A according to the coefficients x1, x2, \u2026, xn.</li> <li>To see how \\(T\\) affects the shape and orientation of objects in \\(R^n\\), we can plot some points or curves in \\(R^n\\) and then apply \\(T\\) to them using matrix multiplication. We can then compare how they look before and after the transformation.</li> </ul> <p>To understand more, you can use a website. Turn on 2 check boxs, \"Show grid\" and \"Show point/vector\". The left side is the \\(R^2\\) and the right side is the \\(R^m\\). Use different matrix \\(A\\) and get a satisfying results.</p>"},{"location":"Statistics/Matrix/#some-examples-of-linear-transformations","title":"Some examples of linear transformations","text":"<p>Here are some examples of linear transformations using matrix and their geometric effects:</p> <ul> <li>Scaling: A scaling transformation changes the size of objects by multiplying them by a constant factor. For example, if A = [2 0; 0 3], then \\(T: R^2 \u2192 R^2\\) defined by \\(T(\\vec{x}) = A\\vec{x}\\) is a scaling transformation that doubles the length of vectors along the x-axis and triples them along the y-axis.</li> <li>Rotation: A rotation transformation rotates objects by a certain angle around a fixed point or axis. For example, if A = [cos\u03b8 -sin\u03b8; sin\u03b8 cos\u03b8], then \\(T: R^2 \u2192 R^2\\) defined by \\(T(\\vec{x}) = A\\vec{x}\\) is a rotation transformation that rotates vectors counterclockwise by an angle \\(\u03b8\\) around the origin.</li> <li>Reflection: A reflection transformation flips objects over a line or plane. For example, if A = [-1 0; 0 1], then \\(T: R^2 \u2192 R^2\\) defined by \\(T(\\vec{x}) = A\\vec{x}\\) is a reflection transformation that reflects vectors over the y-axis.</li> <li>Shear: A shear transformation slants objects by a certain amount along a direction. For example, if A = [1 k; 0 1], then \\(T: R^2 \u2192 R^2\\) defined by \\(T(\\vec{x}) = A\\vec{x}\\) is a shear transformation that shifts vectors by k times their y-coordinate along the x-axis.</li> </ul> <p>These are some of the basic linear transformations using matrix. There are many more that can be obtained by combining or modifying these transformations. For example, a projection transformation maps objects onto a line or plane by discarding some information. A dilation transformation changes the size of objects by multiplying them by different factors along different directions. A squeeze transformation reduces the dimension of objects by mapping them onto a lower-dimensional subspace.</p>"},{"location":"Statistics/Matrix/#eigen-vectors-eigen-values-in-linear-transformation","title":"Eigen Vectors &amp; Eigen Values in Linear Transformation","text":"<p>An eigenvalue of a linear transformation is a scalar \\(\u03bb\\) such that there exists a nonzero vector \\(v\\) that satisfies the equation</p> \\[T(v) = \u03bbv\\] <p>An eigenvector of a linear transformation is a nonzero vector \\(v\\) that satisfies the above equation for some eigenvalue \\(\u03bb\\). In other words, an eigenvector is a vector that does not change direction when the transformation is applied to it, but only changes its length by a factor of \u03bb. If you go the website and if you apply matrix A = [3 1; 0 2], the vector [1, 0] of the left side will be changed to the [3, 0] of the right side coordinate system. So, the eigen vector is [3, 0] and the eigen value is 3 because it makes 3 times to the [1, 0] vector.</p> <p> </p> <p>You will gen another Eigen Vector and Eigen value if your use a vector [-1, 1] to the left side coordinatew. In a 2D coordinate system, there should be 2 eigen vectors and eigen values.</p> <p> </p> <p>The prefix eigen- comes from the German word for \u201cown\u201d or \u201ccharacteristic\u201d. An eigenvalue and an eigenvector describe an intrinsic or characteristic property of a linear transformation.</p>"},{"location":"Statistics/Matrix/#how-to-find-eigen-vectors-and-eigen-values","title":"How to find Eigen Vectors and Eigen Values?","text":"<p>To find the eigenvalues and eigenvectors of a linear transformation \\(T: R^n \u2192 R^n\\) with respect to the standard basis, we need to find the eigenvalues and eigenvectors of its matrix representation A. This can be done by solving the equation</p> \\[ A\\vec{x} = \\lambda{\\vec{x}} \\] <p>for any nonzero vector \\(\\vec{x}\\) in \\(R^n\\) and any scalar \u03bb. You can write the above equation like</p> \\[ A.\\vec{x} = \\lambda.I.\\vec{x} \\Rightarrow (A-\\lambda I).\\vec{x} = 0 \\] <p>where, A is a \\(m \\times n\\) matrix, \\(\\vec{x}\\) is a \\(n \\times 1\\) vector and I is a \\(n \\times n\\) identity matrix.</p> <p>This equation has a nontrivial solution \\((\\vec{x} \\neq 0)\\) if and only if the matrix \\(A - \u03bbI\\) is singular (not invertible). This means that its determinant must be zero. Therefore, we can find the eigenvalues by solving the equation:</p> \u2223A\u2212\u03bbI\u2223=0|A - \\lambda I| = 0  <p>This equation is called the characteristic equation of \\(A\\). It is a polynomial equation of degree \\(n\\) in \\(\u03bb\\). The roots of this equation are the eigenvalues of \\(A\\) (and \\(T\\)). To find the eigenvectors corresponding to each eigenvalue, we need to plug in each eigenvalue into the equation</p> \\[ (A - \\lambda I)\\vec{x} = 0 \\] <p>and find all nonzero solutions for \\(\\vec{x}\\). This can be done by using methods such as Gaussian elimination or Cramer\u2019s rule.</p> <p>The set of all eigenvectors corresponding to an eigenvalue \\(\u03bb\\), together with the zero vector, forms a subspace of \\(R^n\\) called the eigenspace of \\(\u03bb\\). The dimension of this subspace is called the geometric multiplicity of \u03bb.</p>"},{"location":"Statistics/Matrix/#hands-on-calculation-to-find-eigen-vectors-and-eigen-values","title":"Hands on calculation to find Eigen Vectors and Eigen Values","text":"<p>Let us consider an example of finding the eigenvalues and eigenvectors of a 2 x 2 matrix that represents a 2D coordinate system transformation. Suppose we have the matrix:</p> A=[3\u221224\u22121]A = \\begin{bmatrix} 3 &amp; -2 \\\\ 4 &amp; -1 \\end{bmatrix}  <p>To find the eigenvalues of A, we need to solve the characteristic equation</p> \u2223A\u2212\u03bbI\u2223=0|A - \\lambda I| = 0  <p>Expanding this equation, we get:</p> \\[ \\begin{vmatrix} 3-\\lambda &amp; -2\\\\ 4 &amp; -1-\\lambda \\end{vmatrix} = 0 \\] \\[ (3-\\lambda)(-1-\\lambda)+8 = 0 \\] \\[ \\lambda^2 - 2\\lambda + 5 = 0 \\] <p>Using the quadratic formula, we get two solutions for \\(\\lambda\\)</p> \u03bb1=1+2i\\lambda_1 = 1 + 2i  \u03bb2=1\u22122i\\lambda_2 = 1 - 2i  <p>These are the eigenvalues of \\(A\\). Note that they are complex numbers, which means that A does not have any real eigenvectors.</p> <p>To find the eigenvectors corresponding to each eigenvalue, we need to plug in each eigenvalue into the equation:</p> (A\u2212\u03bbI)v=0(A - \\lambda I)v = 0  <p>and find all nonzero solutions for \\(v\\).</p> <p>For \\(\u03bb_1 = 1 + 2i\\), we get</p> [3\u2212(1+2i)\u221224\u22121\u2212(1+2i)]v=0\\begin{bmatrix} 3 - (1 + 2i) &amp; -2 \\\\ 4 &amp; -1 - (1 + 2i) \\end{bmatrix}v = 0  <p>Simplifying this equation, we get</p> [2\u22122i\u221224\u22122\u22122i]v=0\\begin{bmatrix} 2 - 2i &amp; -2 \\\\ 4 &amp; -2 - 2i \\end{bmatrix}v = 0  <p>Using Gaussian elimination, we can reduce this matrix to row echelon form</p> [1\u2212i\u2212100]v=0\\begin{bmatrix} 1 - i &amp; -1 \\\\ 0 &amp; 0 \\end{bmatrix}v = 0  <p>From this matrix, we can see that the first component of v must satisfy</p> (1\u2212i)v1\u2212v2=0(1 - i)v_1 - v_2 = 0  <p>We can choose any nonzero value for \\(v_2\\) and solve for \\(v_1\\). For example, if we choose \\(v_2 = 1\\), we get</p> v1=11\u2212i=1+i2v_1 = \\frac{1}{1 - i} = \\frac{1 + i}{2}  <p>Therefore, one possible eigenvector corresponding to \\(\\lambda_1\\) is</p> v1=[1+i21]v_1 = \\begin{bmatrix} \\frac{1 + i}{2} \\\\ 1 \\end{bmatrix}  <p>We can check that this vector satisfies the equation \\(Av_1 = \\lambda{_1}v_1\\)</p> [3\u221224\u22121][1+i21]=[3+3i22+2i]=(1+2i)[1+i21]\\begin{bmatrix} 3 &amp; -2 \\\\ 4 &amp; -1 \\end{bmatrix}\\begin{bmatrix} \\frac{1 + i}{2} \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{3 + 3i}{2} \\\\ 2 + 2i \\end{bmatrix} = (1 + 2i)\\begin{bmatrix} \\frac{1 + i}{2} \\\\ 1 \\end{bmatrix}  <p>For \\(\\lambda_2 = 1 - 2i\\), we get</p> [3\u2212(1\u22122i)\u221224\u22121\u2212(1\u22122i)]v=0\\begin{bmatrix} 3 - (1 - 2i) &amp; -2 \\\\ 4 &amp; -1 - (1 - 2i) \\end{bmatrix}v = 0  <p>Simplifying this equation, we get</p> [2+2i\u221224\u22122+2i]v=0\\begin{bmatrix} 2 + 2i &amp; -2 \\\\ 4 &amp; -2 + 2i \\end{bmatrix}v = 0  <p>Using Gaussian elimination, we can reduce this matrix to row echelon form</p> [1+i\u2212100]v=0\\begin{bmatrix} 1 + i &amp; -1 \\\\ 0 &amp; 0 \\end{bmatrix}v = 0  <p>From this matrix, we can see that the first component of v must satisfy</p> (1+i)v1\u2212v2=0(1 + i)v_1 - v_2 = 0  <p>We can choose any nonzero value for \\(v_2\\) and solve for \\(v_1\\). For example, if we choose \\(v_2 = 1\\), we get</p> v1=11+i=1\u2212i2v_1 = \\frac{1}{1 + i} = \\frac{1 - i}{2}  <p>Therefore, one possible eigenvector corresponding to \\(\\lambda_2\\) is</p> v2=[1\u2212i21]v_2 = \\begin{bmatrix} \\frac{1 - i}{2} \\\\ 1 \\end{bmatrix}  <p>We can check that this vector satisfies the equation \\(Av_2 = \\lambda_{2}v_2\\)</p> [3\u221224\u22121][1\u2212i21]=[3\u22123i22\u22122i]=(1\u22122i)[1\u2212i21]\\begin{bmatrix} 3 &amp; -2 \\\\ 4 &amp; -1 \\end{bmatrix}\\begin{bmatrix} \\frac{1 - i}{2} \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{3 - 3i}{2} \\\\ 2 - 2i \\end{bmatrix} = (1 - 2i)\\begin{bmatrix} \\frac{1 - i}{2} \\\\ 1 \\end{bmatrix}"},{"location":"Statistics/Matrix/#properties-of-eigen-vectors-and-eigen-values","title":"Properties of Eigen Vectors and Eigen Values","text":"<ul> <li> <p>Sum of Eigen values: The sum of all the eigen values of a matrix is equal to its trace (the sum of the diagonal elements of the matrix). This holds true regardless of whether the matrix is square or not.</p> </li> <li> <p>Product of Eigen values: The product of all the eugen values of a matrix is equal to its determinant. This holds true for square matrix.</p> </li> <li> <p>Eigen vectors corresponsing to different eigen values are orthogonal: If a matrix \\(A\\) is symmetric (i.e., \\(A=A6T\\)), the eigen vectors corresponding to distinct eigen values are orthogonal to each other.</p> </li> <li> <p>Eigen value of an Identity matrix: For an Identity matrix, the eigen values are all 1, regardless of the dimension of the matrix.</p> </li> <li> <p>Eigen value of a scalar multiple: If \\(B\\) is a matrix obtained by multiplying a scalar \\(c\\) to a matrix \\(A\\) (i.e., \\(B = cA\\)), then the eigen values of \\(B\\) are just the eigen values of \\(A\\) each multiplied by \\(c\\).</p> </li> <li> <p>Eigen values of a Diagonal Matrix: For a diagonal matrix, the eigen values are the diagonal elements themselves.</p> </li> <li> <p>Eigen values of a Transposed Matrix: The eigen values of a matrix and it's transpose are the same.</p> </li> </ul>"},{"location":"Statistics/One-Way-ANOVA/","title":"Understanding One Way ANOVA","text":"<p>In the realm of statistical analysis, ANOVA (Analysis of Variance) is a powerful tool that allows researchers to uncover significant differences between multiple groups or treatments. Whether you're conducting a scientific study, analyzing market research data, or simply curious about statistical methodologies, understanding ANOVA can provide valuable insights into the relationships and patterns within your data.</p>"},{"location":"Statistics/One-Way-ANOVA/#what-is-anova","title":"What is ANOVA?","text":"<p>ANOVA is a statistical technique that compares the means of two or more groups to determine if there are any significant differences among them. It assesses the variability within and between the groups to draw meaningful conclusions about the factors influencing the observed differences. ANOVA is particularly useful when you want to test the null hypothesis that there are no significant differences between the groups.</p>"},{"location":"Statistics/One-Way-ANOVA/#types-of-anova","title":"Types of ANOVA","text":"<p>There are various types of ANOVA, each suited for different scenarios. The most common types include one-way ANOVA, two-way ANOVA, and repeated measures ANOVA. One-way ANOVA analyzes the effects of a single factor, while two-way ANOVA examines the influence of two factors simultaneously. Repeated measures ANOVA is used when data is collected from the same subjects over multiple time points or conditions.</p> <pre><code>graph TD\n  A[ANOVA] --&gt; B{One Way ANOVA};\n  A --&gt; C{Two Way ANOVA};\n  A --&gt; D{Repeated Measures ANOVA};</code></pre>"},{"location":"Statistics/One-Way-ANOVA/#f-distribution","title":"F-Distribution","text":"<p>Before starting the ANOVA, we have to learn F-Distribution because this statistical tests are based on the F - Distribution. The F distribution is a probability distribution that arises in the context of statistical inference, particularly in analysis of variance (ANOVA) and regression analysis. It is named after the statistician Sir Ronald Fisher, who developed many fundamental concepts in statistics.</p> <p></p> <p>This is a continuous probability distribution by using the Chi-Square Distribution \\((\u03c7^2)\\) and every Chi-Square distribution has one one parameter - degrees of freedom (\\(df\\)).</p> <p>Lets assume, we have 2 Chi - Square Distribution \\((\u03c7^2_1)\\) &amp; \\((\u03c7^2_2)\\) and their degrees of freedoms are \\(df_1\\) and \\(df_2\\) respectively. Then</p> \\[F-Distribution = \\frac{\u03c7^2_1/df_1}{\u03c7^2_2/df_2}\\] <p>Because of the Chi-Square distribution takes only non-negative values, the F distribution doesn't have non-negative values. It is positively skewed and has different shapes depending on the degrees of freedom associated with it. This is commonly used to test hypothesis about the equality of two variances in different samples or populations. The F distribution is widely used in various fields of research, including psychology, education, economics and the natural and social sciences, for hypothesis testing and model comparison.</p> <p>The F-statistics is calculated by dividing the ratio of two sample variances or mean squares from an ANOVA table. This value is then compared to critical values from the F-distribution to determine statistical significance.</p>"},{"location":"Statistics/One-Way-ANOVA/#one-way-anova","title":"One Way ANOVA","text":"<p>One-way ANOVA (Analysis of Variance) is a statistical technique that allows researchers to compare the means of three or more groups to determine if there are any significant differences among them. This powerful tool is widely used in various fields, from social sciences and psychology to business and healthcare. Understanding the basics of one-way ANOVA can help unravel hidden patterns and group distinctions within your data.</p> <p>One-way ANOVA is a parametric statistical test that examines the variability between multiple groups based on a single independent variable, also known as a factor. The goal is to determine if the observed differences in means across groups are statistically significant or simply due to chance. This type of ANOVA is particularly useful when you want to compare more than two groups simultaneously.</p>"},{"location":"Statistics/One-Way-ANOVA/#working-principle-of-one-way-anova","title":"Working principle of One-way ANOVA","text":"<p>One-way ANOVA works by decomposing the total variability in the data into two components: the variability between groups and the variability within groups. It then calculates an F-statistic by comparing the ratio of between-group variability to within-group variability. The F-statistic follows an F distribution, which is used to assess the statistical significance of the group differences.</p>"},{"location":"Statistics/One-Way-ANOVA/#steps-to-calculate-one-way-anova","title":"Steps to calculate One Way ANOVA","text":"<ol> <li>Define the null (\\(H_0\\)) and alternative hypothesis (\\(H_1\\)). The null hypothesis is that all the group means are equal. The \\(H_1\\) is that at least one group is significantly different from others.</li> <li>Calculate the overall mean (grand mean) of all the groups combined and mean of all the groups individually.</li> <li>Calculate the \"between-group\" and \"within-group\" sum of squares (SS) and their respective degrees of freedom.</li> <li>Calculate the \"between-group\" and \"within-group\" mean squares (MS) by dividing their respective sum of the squares by their degrees of freedom.</li> <li>Calculate the F-Statistics by dividing the \"between-group\" mean square by the \"within-group\" mean square.</li> <li>By using the F-Statistics, calculate the P-Value.</li> <li>By comparing the P-value with your threshold (\\(\\alpha\\)), either accept or reject the Null Hypothesis.</li> </ol> <p>This calculation can be represented by The ANOVA Table</p> Source of variance Sums of Squares Degrees of freedom Mean square F-Statistics P-value Between groups \\(SS_{b}\\) k-1 \\(MS_{b}\\) \\(MS_b/MS_w\\) p Within groups \\(SS_{w}\\) N-k \\(MS_{w}\\) - - Total \\(SS_{t}\\) N-1 - - -"},{"location":"Statistics/One-Way-ANOVA/#hands-on-example","title":"Hands on Example","text":"<p>Let's see we have a dataset like the below one. We have 3 groups (A, B and C) and for each group has different numerical values.</p> <p> A B C 3 1 8 6 8 6 3 9 10 <p></p> <p>You can assume the \"Pclass\" and \"Age\" column from the famous titanic dataset. The \"Pclass\" has 3 categories e.g., class-1, class-2 and class-3 and the numericals are the \"Age\" column values. The \"Pclass\" is called factors and categories/groups are called levels. For every level can have different number of values. For each value of the levels are called individuals.</p> <ul> <li>Step-1: Define the \\(H_0\\) hypothesis.</li> </ul> \\[H_0 = \\mu_A =\\mu_B = \\mu_C\\] <p>Define the \\(H_1\\). \\(H_1 =\\) At-least one mean is significantly different.</p> <ul> <li>Step-2: Now, for every group we have to find the mean. So,</li> </ul> \\[\\overline{X_A} = 4; \\ \\overline{X_B} = 6; \\ \\overline{X_C} = 8\\] <p>And the grand mean is</p> \\[\\overline{X} = \\frac{\\overline{X_A} + \\overline{X_B} + \\overline{X_C}}{3} = 6\\] <ul> <li>Step-3: Now time to calculate the SS (Sum of Squares).</li> </ul> \\[\\large SS_{total} = \\sum_{j=1}^{levels}\\sum_{i=1}^{individuals}(\\overline{X_{ij}} - \\overline{X})^2 = 76\\] <p>and the degrees of freedom is</p> \\[\\large df_{total} = N - 1 = 9 - 1 = 8\\] <p>where, \\(N\\) is the total individuals.</p> <p>Now we will calculate the Sum of Squares Between.</p> <p>$$ \\large SS_{between} = \\sum_{j=1}^{levels}(\\overline{X_j} - \\overline{X})^2*n_j = 24 $$.</p> \\[ \\large df_{between} = k - 1 = 3 - 1 = 2 \\] <p>Where \\(k\\) is the number of levels.</p> <p>We can also calculate the Sum of Squares Within.</p> \\[ \\large SS_{within} =  \\sum_{j=1}^{levels}\\sum_{i=1}^{individuals}(X_{ij} - \\overline{X_j})^2 = 52 \\] \\[ \\large df_{within} = N - k = 9 - 3 = 6 \\] <p>You can notice that</p> \\[ \\large SS_{total} = SS_{between} + SS_{within} \\] \\[ \\large df_{total} = df_{between} + SS_{within} \\] <ul> <li>Step-4: Now time to calculate the Mean of Squares from the Sum of Squares.</li> </ul> \\[ \\large MS_{between} = \\frac{SS_{between}}{df_{between}} = \\frac{24}{2} = 12 \\] \\[ \\large MS_{within} = \\frac{SS_{within}}{df_{within}} = \\frac{52}{6} = 8.67 \\] <ul> <li>Step-5: Now time to calculate the F-Statistics.</li> </ul> \\[ \\large F_{k-1, N-k} = \\frac{MS_{between}}{MS_{within}} = \\frac{12}{8.67} = 1.384 \\] <ul> <li>Step-6: By using the F-Statistics value, we will calculate the P-Value. For that you can use the below python code.</li> </ul> <p><pre><code>import scipy.stats as stats\n\nf_statistic = 1.384  # The F-statistic value you've calculated\ndf1 = 2              # Degrees of freedom for the numerator (between groups)\ndf2 = 6              # Degrees of freedom for the denominator (within groups)\n\np_value = stats.f.sf(f_statistic, df1, df2)\nprint(\"P-value:\", p_value)\n</code></pre> And the value is calculated 0.320.</p> <ul> <li>Step-7: The P-value is much greater than the significant value (\\(\\alpha = 0.05\\)). So, we can't find enough evidence to reject the null hypothesis. That's why we can say that the groups/levels are taken from the same population.</li> </ul>"},{"location":"Statistics/One-Way-ANOVA/#assumptions-of-one-way-anova","title":"Assumptions of One Way ANOVA","text":"<ol> <li> <p>Independence: The observations within and between groups should be independent of each other. This means that the outcome of one observation should not influence the outcome of another. Independence is typically achieved through random sampling or random assignment of subjects to groups.</p> </li> <li> <p>Normality: The data within each group should be approximately normally distributed. While one-way ANOVA is considered to be robust to moderate violations of normality, severe deviations may affect the accuracy of the test results. If normality is in doubt, non-parametric alternatives like the Shapiro-Wilk test can be considered.</p> </li> <li> <p>Homogeneity of variances: The variances of the populations from which the samples are drawn should be equal, or at-least approximately so. This assumption is known as homoscedasticity. If the variances are substantially different, the accuracy of the test results may be compromised. Levene's test or Bartlett's test can be used to assess the homogeneity of variance. If this assumption is violated, alternative tests such as Welch's ANOVA can be used.</p> </li> </ol>"},{"location":"Statistics/One-Way-ANOVA/#practical-example-with-code","title":"Practical example with code","text":"<pre><code># import the necessary libraries\nimport seaborn as sns\nimport pingouin as pg  # pip install pingouin\n\n# load the famous titanic dataset\ndf = sns.load_dataset('titanic')\n\n# calculate the ANOVA table\nanova_table = pg.anova(data=df, dv='age', between='pclass', detailed=True)\n\nprint(anova_table)\n</code></pre> <p>The result of the above is like this:</p> Source SS DF MS F p-unc np2 pclass 20929.627754 2 10464.813877 57.443484 7.487984e-24 0.139107 Within 129527.008190 711 182.175820 NaN NaN NaN <p>The p-unc is the P-value of this ANOVA test. You can see that the P-value is very less compared to the significant level (\\(\\alpha = 0.05\\)). So we can get enough evidence to reject the Null Hypothesis (\\(H_0\\)). That's mean atleast one of the category of the pclass is significantly different from the others. But the ANOVA test can't tell which group(s) is/are significantly different from others. To know which group(s) is/are significantly different, we have to do Post-Hoc test. </p>"},{"location":"Statistics/One-Way-ANOVA/#post-hoc-test","title":"Post-Hoc Test","text":"<p>While ANOVA determines if there are significant differences between groups, it does not reveal the specific pairings of groups that differ. Post hoc tests address this limitation by allowing researchers to make multiple pairwise comparisons. These tests provide a more detailed understanding of the group differences, enabling researchers to draw more precise conclusions. Generally, it has two types:</p> <ol> <li>Bonferroni correction</li> <li>Tukey's HSD (Honestly Significant Differece) Test</li> </ol>"},{"location":"Statistics/One-Way-ANOVA/#bonferroni-correction","title":"Bonferroni correction","text":"<p>In this test, perform all the possible t-test and then compare the result. Or, you can do few number of tests and compare them. This method adjusts the significance level (\\(\\alpha\\)) by dividing it by the number of comparisons being made. It is a conservation The Bonferroni correction is a conservative approach that adjusts the significance level for each comparison to maintain the overall family-wise error rate. This method divides the desired alpha level by the number of pairwise comparisons. While effective in controlling Type I error, the Bonferroni correction may be overly stringent and increase the chance of Type II error.</p> <pre><code>import scipy.stats as stats\n\nfor class1, class2 in [(1,2), (2, 3), (3, 1)]:\n    print(f\"Class {class1} vs Class {class2}\")\n    print(stats.ttest_ind(df[df['pclass'] == class1]['age'].dropna(),\n                          df[df['pclass'] == class2]['age'].dropna()))\n    print()\n</code></pre> <p>And the result is like this</p> <pre><code>Class 1 vs Class 2\nTtest_indResult(statistic=5.485187676773201, pvalue=7.835568991415144e-08)\n\nClass 2 vs Class 3\nTtest_indResult(statistic=3.927800191020872, pvalue=9.715078600777852e-05)\n\nClass 3 vs Class 1\nTtest_indResult(statistic=-10.849122601201033, pvalue=6.134470007830625e-25)\n</code></pre> <p>If we see the result, then the Class 1 vs Class 2 test and Class 3 vs Clas 1 test results are extremly significant. So, the Class 1 of \"pclass\" is significantly different from the others. But if we take significant level \\(\\alpha = \\frac{0.05}{3} = 0.0167\\) then all the groups are significantly different from each other.</p>"},{"location":"Statistics/One-Way-ANOVA/#tukeys-hsd-test","title":"Tukey's HSD Test","text":"<p>Tukey's HSD test is widely used and provides simultaneous confidence intervals for all possible pairwise comparisons. It controls the family-wise error rate, making it a popular choice. Tukey's HSD test is suitable for equal and unequal sample sizes.</p> <pre><code># import the required libraries\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# select our columns and drop the NaN values\nrequired_df = df[['age', 'pclass']].dropna()\ntukey = pairwise_tukeyhsd(endog=required_df['age'], groups=required_df['pclass'], alpha=0.05)\n\ntukey.plot_simultaneous()\n\nplt.show()\n</code></pre> <p>The result of the above code is</p> <p></p> <p>You can also see the summary of this test by running the below code.</p> <pre><code>pd.DataFrame(tukey.summary())\n</code></pre> <p>The result is</p> <p></p>"},{"location":"Statistics/One-Way-ANOVA/#disadvantages-of-one-way-anova","title":"Disadvantages of One Way ANOVA","text":"<p>While one-way ANOVA is a useful statistical technique, it also has certain drawbacks that researchers should be aware of. Here are some common drawbacks of one-way ANOVA:</p> <p>Assumption of Homogeneity of Variances: One-way ANOVA assumes that the variances across all groups are equal. Violation of this assumption, known as heteroscedasticity, can affect the accuracy and validity of the results. It may lead to inflated or deflated Type I error rates, making it challenging to draw accurate conclusions.</p> <p>Sensitivity to Outliers: One-way ANOVA can be sensitive to outliers, particularly when the sample sizes are small. Outliers can significantly impact the mean and variance, potentially influencing the results and leading to incorrect interpretations.</p> <p>Assumption of Normality: One-way ANOVA assumes that the data within each group follows a normal distribution. If the assumption is violated, it can affect the validity of the results. However, one-way ANOVA is often robust to violations of normality, especially when the sample sizes are large.</p> <p>Limited to One Independent Variable: As the name suggests, one-way ANOVA is designed to analyze the effects of a single independent variable or factor on a dependent variable. It is not suitable for investigating the influence of multiple independent variables or complex relationships between variables. In such cases, other statistical techniques like factorial ANOVA or regression analysis may be more appropriate.</p> <p>Post-hoc Interpretation: While one-way ANOVA can identify significant group differences, it does not provide specific information about which groups differ from each other. Post-hoc tests are often necessary to make pairwise comparisons, but these tests increase the chances of Type I errors. Multiple post-hoc tests also require careful consideration of family-wise error rate control methods.</p> <p>Lack of Causal Inference: One-way ANOVA examines associations and differences between groups, but it does not establish causality. It only provides evidence for the existence of group differences, not the underlying causes or mechanisms responsible for those differences. Causal relationships may require additional experimental designs or more sophisticated statistical analyses.</p> <p>Sample Size Considerations: One-way ANOVA performs better with larger sample sizes. Small sample sizes can limit the statistical power of the analysis, making it difficult to detect significant group differences accurately. It is crucial to ensure an adequate sample size to increase the reliability and generalizability of the findings.</p>"},{"location":"Statistics/One-Way-ANOVA/#applications-of-anova","title":"Applications of ANOVA","text":"<p>Hyperparameter tuning: When selecting the best hyperparameters for a machine learning model, one-way ANOVA can be used to compare the performance of models with different hyperparameter settings. By treating each hyperparameter settings as a group, you can perform one-way ANOVA to determine if there are any significant differences in performance across the various settings.</p> <p>Feature selection: One-way ANOVA can be used as a univariate feature selection method to idenity features that are significantly associated with the target variable, especially when the target variable is categorical with more than two levels. In this context, the one-way ANOVA is performed for each feature, and features with low p-values are considered to be more relevant for prediction.</p> <p>Algorithm comparison: When comparing the performance of different machine learning algorithms, one-way ANOVA can be used to determine if there are any significant differences in their performance metrics (e.g., accuracy, F1 score, etc.) across multiple runs or cross-validation folds. This can help you decide which algorithm is the most suitable for a specific problem.</p> <p>Model stability assessment: One-way ANOVA can be used to assess the stability of a machine learning model by comparing its performance across different random seeds or initializations. If the model's performance varies significantly between different initializations, it may indicate that the model is unstable or highly sensitive to the choice of initial conditions.</p>"},{"location":"Statistics/One-Way-ANOVA/#why-t-test-is-not-used","title":"Why t-test is not used","text":""}]}